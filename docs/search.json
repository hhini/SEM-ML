[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SEM + ML 实际项目论文复现",
    "section": "",
    "text": "要具体知道为什么要结合，可以参考文献 混合人工神经网络与结构方程模型技术：综述Hybrid artificial neural network and structural equation modelling techniques: a survey\n\n\n\n\n结构方程模型（SEM）是一种强大的因果建模工具，能够同时处理测量模型（潜变量）和路径模型（变量间关系）。但在实际研究中，尤其是面对高维数据(就是变量多)、复杂变量结构或非线性关系时，SEM也面临挑战：\n\n变量筛选困难：传统SEM依赖理论假设，变量选择主观性强(一般要参考社会科学文献，并且进行假设，麻了)；\n模型拟合受限：当变量数量多、关系复杂时，SEM模型容易过拟合或拟合不佳(过拟合和欠拟合)；\n预测能力有限：SEM更偏向解释性而非预测性。\n\n而机器学习（ML）正好可以补足这些短板：\n\n\n\n功能\nSEM\nML\n\n\n\n\n因果推理\n✅ 强\n❌ 弱（但可辅助）\n\n\n潜变量建模\n✅ 支持\n❌ 不直接支持\n\n\n特征筛选\n❌ 依赖理论\n✅ 数据驱动\n\n\n非线性建模\n❌ 有限\n✅ 强大\n\n\n预测能力\n❌ 弱\n✅ 强\n\n\n\n因此，将ML与SEM结合，可以实现：\n\n数据驱动的变量筛选（如用随机森林或SHAP值找出最重要的预测因子(一般来说如果变量较多的时候，一般是不降维的，而是筛选变量的）；\n提升模型解释力与预测力（先用ML找出关键变量，再用SEM建模路径）；\n增强模型稳健性（ML可用于验证SEM模型的泛化能力）；\n提高研究影响力（结合方法更前沿，易发表在高影响因子期刊）。\n\n\n\n\n以下是提供的几篇代表性文献，展示了ML+SEM的实际应用与发表潜力：\n\n✅ 复现的论文： PMC12236327\n\n结合随机森林与SEM，识别影响精神分裂症患者主观幸福感的关键因素。\n通过ML筛选变量，再用lavaan构建路径模型，实现因果解释与预测并重。\n发表在《Translational Psychiatry》，IF ≈ 7.9。\n\n📱 智能穿戴设备采纳研究： Bou Nassif et al. (2022)\n\n使用PLS-SEM与ML对智能手表采纳意图进行建模。\nML揭示“用户满意度”是最关键预测因子，SEM验证其路径关系。\n发表在《Heliyon》，IF ≈ 4.0。\n\n🧠 消费者心理与行为研究： ScienceDirect 2023论文\n\n用ML筛选影响消费者信任的变量，再用SEM建模信任形成机制。\n发表在《Journal of Business Research》，IF ≈ 10.0。\n\n🧬 健康行为预测研究： ScienceDirect 2024论文\n\n结合ML与SEM预测健康行为采纳，强调数据驱动与理论建模的融合。\n发表在《Preventive Medicine Reports》，IF ≈ 3.5。\n\n\n\n\n\n\n\n在正式进入机器学习（Machine Learning, ML）＋结构方程模型（SEM）的整合应用之前，首先要让大家对“机器学习”这个概念有一个直观的理解。不必过分纠结复杂的数学公式，而要抓住它的核心思想和研究流程。\n\n\n机器学习是一种让计算机通过“样本数据”自行发现规律，并将这些规律用于“预测”或“决策”的技术。\n- 它不像传统统计学那样强调整体假设检验和参数估计，更注重“从数据中自动学习”。\n- ML 旨在用已有数据训练一个模型，让模型在新数据上也能表现出良好的预测或分类能力。\n这也让机器学习无法用数学公式和因果关系来知道电脑是怎么想的，形成了所谓的黑匣子，解释性太差，但是用来预测就没毛病\n\n\n\n\n目标不同\n\n统计学：偏重解释性，解析变量间的因果或关联；\n\n机器学习：偏重预测性，追求模型的泛化能力，即给我数据我可以预测出大概的，符合之前数据的答案。\n\n模型假设\n\n统计模型：通常要求满足正态性、线性关系等严格假设；\n\nML 模型：对数据分布和关系的假设更弱，可以处理非线性、高维度数据。\n\n关注点\n\n统计方法：估计参数、显著性检验、置信区间；\n\nML 方法：模型选择、交叉验证、过拟合控制、预测误差。\n\n\n\n\n\n\n监督学习（Supervised Learning）\n\n有“标签”的数据（例如：患者有/无压力性损伤）。\n\n常见算法：逻辑回归、决策树、随机森林、支持向量机、神经网络等。\n\n核心任务：分类（Classification）和回归（Regression）。\n\n无监督学习（Unsupervised Learning）\n\n无标签数据。\n\n常见算法：聚类（K-means、层次聚类）、降维（PCA、t-SNE）。\n\n核心任务：发现数据的内在结构、分群、特征降维。\n\n强化学习（Reinforcement Learning）\n\n通过“试错”与环境互动来学习最优策略。\n\n在护理研究中应用相对少，可用于优化护理路径或资源调度。\n\n\n\n\n\n\n数据准备与清洗\n\n汇总临床指标、问卷量表、电子健康记录等；\n\n处理缺失值、异常值、分类变量编码。\n\n特征工程\n\n选择或构造与研究目标最相关的变量（特征）即挑选重要的特征变量，过滤不需要的变量；\n\n归一化、标准化、one-hot 编码(独热编码)等。\n\n模型训练与优化\n\n在“训练集”上训练模型；\n\n通过交叉验证（Cross-Validation）选择超参数，防止过拟合(过拟合即是对模型来说，学习太多不重要的特征，比如我训练了一个识别学长还是学姐的模型，但是模型学习到了几个女装的学长，本来女装这是几个人的特殊癖好，但是模型认为女装的都是学长，这扯不扯）。\n\n模型评估\n\n在“测试集”或“留出数据”上评估预测性能，这也是为什么机器学习都要分成训练集和验证集，要么7/3,要么8/2等等（千万别耍小聪明，觉得数据不够多，还要拆分，耍滑头把所有数据都拿去训练，然后再拿其中一部分来预测，因为模型有记忆的，你预测训练过的数据，肯定准确率是百分百，就好像你明明知道明天的天气，那为什么要根据之前的数据推测呢）（如 AUC、准确率、RMSE）；\n\n比较不同算法的优劣。\n\n模型解释与应用\n\n使用 SHAP、LIME 等方法解释模型\n它们是两种常用的“模型解释工具”，可以告诉我们：哪些变量对模型预测最重要，以及它们的影响方向"
  },
  {
    "objectID": "index.html#为什么要结合机器学习ml与结构方程模型sem",
    "href": "index.html#为什么要结合机器学习ml与结构方程模型sem",
    "title": "SEM + ML 实际项目论文复现",
    "section": "",
    "text": "结构方程模型（SEM）是一种强大的因果建模工具，能够同时处理测量模型（潜变量）和路径模型（变量间关系）。但在实际研究中，尤其是面对高维数据(就是变量多)、复杂变量结构或非线性关系时，SEM也面临挑战：\n\n变量筛选困难：传统SEM依赖理论假设，变量选择主观性强(一般要参考社会科学文献，并且进行假设，麻了)；\n模型拟合受限：当变量数量多、关系复杂时，SEM模型容易过拟合或拟合不佳(过拟合和欠拟合)；\n预测能力有限：SEM更偏向解释性而非预测性。\n\n而机器学习（ML）正好可以补足这些短板：\n\n\n\n功能\nSEM\nML\n\n\n\n\n因果推理\n✅ 强\n❌ 弱（但可辅助）\n\n\n潜变量建模\n✅ 支持\n❌ 不直接支持\n\n\n特征筛选\n❌ 依赖理论\n✅ 数据驱动\n\n\n非线性建模\n❌ 有限\n✅ 强大\n\n\n预测能力\n❌ 弱\n✅ 强\n\n\n\n因此，将ML与SEM结合，可以实现：\n\n数据驱动的变量筛选（如用随机森林或SHAP值找出最重要的预测因子(一般来说如果变量较多的时候，一般是不降维的，而是筛选变量的）；\n提升模型解释力与预测力（先用ML找出关键变量，再用SEM建模路径）；\n增强模型稳健性（ML可用于验证SEM模型的泛化能力）；\n提高研究影响力（结合方法更前沿，易发表在高影响因子期刊）。\n\n\n\n\n以下是提供的几篇代表性文献，展示了ML+SEM的实际应用与发表潜力：\n\n✅ 复现的论文： PMC12236327\n\n结合随机森林与SEM，识别影响精神分裂症患者主观幸福感的关键因素。\n通过ML筛选变量，再用lavaan构建路径模型，实现因果解释与预测并重。\n发表在《Translational Psychiatry》，IF ≈ 7.9。\n\n📱 智能穿戴设备采纳研究： Bou Nassif et al. (2022)\n\n使用PLS-SEM与ML对智能手表采纳意图进行建模。\nML揭示“用户满意度”是最关键预测因子，SEM验证其路径关系。\n发表在《Heliyon》，IF ≈ 4.0。\n\n🧠 消费者心理与行为研究： ScienceDirect 2023论文\n\n用ML筛选影响消费者信任的变量，再用SEM建模信任形成机制。\n发表在《Journal of Business Research》，IF ≈ 10.0。\n\n🧬 健康行为预测研究： ScienceDirect 2024论文\n\n结合ML与SEM预测健康行为采纳，强调数据驱动与理论建模的融合。\n发表在《Preventive Medicine Reports》，IF ≈ 3.5。"
  },
  {
    "objectID": "index.html#什么是机器学习",
    "href": "index.html#什么是机器学习",
    "title": "SEM + ML 实际项目论文复现",
    "section": "",
    "text": "在正式进入机器学习（Machine Learning, ML）＋结构方程模型（SEM）的整合应用之前，首先要让大家对“机器学习”这个概念有一个直观的理解。不必过分纠结复杂的数学公式，而要抓住它的核心思想和研究流程。\n\n\n机器学习是一种让计算机通过“样本数据”自行发现规律，并将这些规律用于“预测”或“决策”的技术。\n- 它不像传统统计学那样强调整体假设检验和参数估计，更注重“从数据中自动学习”。\n- ML 旨在用已有数据训练一个模型，让模型在新数据上也能表现出良好的预测或分类能力。\n这也让机器学习无法用数学公式和因果关系来知道电脑是怎么想的，形成了所谓的黑匣子，解释性太差，但是用来预测就没毛病\n\n\n\n\n目标不同\n\n统计学：偏重解释性，解析变量间的因果或关联；\n\n机器学习：偏重预测性，追求模型的泛化能力，即给我数据我可以预测出大概的，符合之前数据的答案。\n\n模型假设\n\n统计模型：通常要求满足正态性、线性关系等严格假设；\n\nML 模型：对数据分布和关系的假设更弱，可以处理非线性、高维度数据。\n\n关注点\n\n统计方法：估计参数、显著性检验、置信区间；\n\nML 方法：模型选择、交叉验证、过拟合控制、预测误差。\n\n\n\n\n\n\n监督学习（Supervised Learning）\n\n有“标签”的数据（例如：患者有/无压力性损伤）。\n\n常见算法：逻辑回归、决策树、随机森林、支持向量机、神经网络等。\n\n核心任务：分类（Classification）和回归（Regression）。\n\n无监督学习（Unsupervised Learning）\n\n无标签数据。\n\n常见算法：聚类（K-means、层次聚类）、降维（PCA、t-SNE）。\n\n核心任务：发现数据的内在结构、分群、特征降维。\n\n强化学习（Reinforcement Learning）\n\n通过“试错”与环境互动来学习最优策略。\n\n在护理研究中应用相对少，可用于优化护理路径或资源调度。\n\n\n\n\n\n\n数据准备与清洗\n\n汇总临床指标、问卷量表、电子健康记录等；\n\n处理缺失值、异常值、分类变量编码。\n\n特征工程\n\n选择或构造与研究目标最相关的变量（特征）即挑选重要的特征变量，过滤不需要的变量；\n\n归一化、标准化、one-hot 编码(独热编码)等。\n\n模型训练与优化\n\n在“训练集”上训练模型；\n\n通过交叉验证（Cross-Validation）选择超参数，防止过拟合(过拟合即是对模型来说，学习太多不重要的特征，比如我训练了一个识别学长还是学姐的模型，但是模型学习到了几个女装的学长，本来女装这是几个人的特殊癖好，但是模型认为女装的都是学长，这扯不扯）。\n\n模型评估\n\n在“测试集”或“留出数据”上评估预测性能，这也是为什么机器学习都要分成训练集和验证集，要么7/3,要么8/2等等（千万别耍小聪明，觉得数据不够多，还要拆分，耍滑头把所有数据都拿去训练，然后再拿其中一部分来预测，因为模型有记忆的，你预测训练过的数据，肯定准确率是百分百，就好像你明明知道明天的天气，那为什么要根据之前的数据推测呢）（如 AUC、准确率、RMSE）；\n\n比较不同算法的优劣。\n\n模型解释与应用\n\n使用 SHAP、LIME 等方法解释模型\n它们是两种常用的“模型解释工具”，可以告诉我们：哪些变量对模型预测最重要，以及它们的影响方向"
  },
  {
    "objectID": "index.html#改进模型",
    "href": "index.html#改进模型",
    "title": "SEM + ML 实际项目论文复现",
    "section": "改进模型🤖🤖",
    "text": "改进模型🤖🤖\n虽然我们的模型AUC有0.8左右，这样的数据已经不错了，如果比较原来的论文原论文链接\nThe random forest (RF) model had the highest area under the curve (AUC) of 0.794 at baseline. Obsessive-compulsive symptoms (OCS) had the most significant impact on high levels of SWB, followed by somatization, cognitive deficits, and depression. The network analysis demonstrated robust connections among the SWB, OCS, and somatization. SEM analysis revealed that OCS exerted the strongest direct effect on SWB, and also an indirect effect via the mediation of depression. Furthermore, the contribution of OCS at baseline to SWB was maintained 6 months later.随机森林 (RF) 模型的基线曲线下面积 (AUC) 最高，为 0.794。强迫症状 (OCS) 对高水平主观幸福感 (SWB) 的影响最为显著，其次是躯体化、认知缺陷和抑郁症。网络分析表明，SWB、OCS 和躯体化之间存在稳态联系。结构方程模型 (SEM) 分析显示，OCS 对 SWB 的直接影响最为显著，并且也通过抑郁症的中介产生间接影响。此外，基线 OCS 对 SWB 的贡献在 6 个月后得以维持。\n看起来确实已经不错了，但是我们还是可以对原来的模型进行改善，这是原论文没有进行的操作，具体有两种方法，在模型不改变的情况下\n特征选择（Feature Selection） 和 超参数调整（Hyperparameter Tuning）\n\n🧠 为什么要进行特征选择？\n\n理念：少即是多（Less is More）\n\n在机器学习中，使用所有变量并不总是最好的选择。有些变量可能：\n\n与目标无关（噪声）；\n与其他变量高度冗余（重复信息）；\n增加模型复杂度，导致过拟合；\n降低模型解释性，让临床人员难以理解。\n\n\n\n✅ 特征选择的好处：\n\n提升模型性能：去除无关变量后，模型更专注于有效信息；\n提高训练速度：变量少，计算更快；\n增强解释性：更容易向临床人员解释模型决策；\n减少过拟合风险：模型更稳健，泛化能力更强。\n\n\n🔧 为什么要进行超参数调整？\n\n理念：模型的“个性化设置”决定它的表现\n\n每个机器学习模型都有一些“内部设置”，称为超参数（hyperparameters），它们不是从数据中学出来的，而是你提前设定的。\n例如：\n\n随机森林的 n_estimators（树的数量）、max_depth（树的最大深度）；\nXGBoost 的 learning_rate（学习率）、subsample（样本采样比例）；\nLightGBM 的 num_leaves（叶子数）、min_data_in_leaf（每个叶子最少样本数）。\n\n\n\n\n✅ 超参数调整的好处：\n\n提升模型性能：找到最适合当前数据的参数组合；\n控制模型复杂度：防止过拟合或欠拟合；\n增强模型稳定性：提高在不同数据划分下的表现一致性。\n\n\n\n特征选择🤩\n\n# 导入所需的库\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\n# --- 2. 识别最重要的特征 ---\n# Identify the most important features\n\n#和以前一样导入数据\n# --- 1. 加载并准备数据 (与之前相同) ---\n# Load and Prepare the Data\n\n# !!! 重要提示: 请将下面的路径修改为您保存 CSV 文件的实际位置 !!!\nfile_path = \"./synthetic_schizophrenia_data.csv\"\ndf = pd.read_csv(file_path)\n\ndf['High_SWB'] = (df['SWN_Total_Score'] &gt;= 80).astype(int)\nX = df.drop(['SWN_Total_Score', 'High_SWB'], axis=1)\ny = df['High_SWB']\nX = pd.get_dummies(X, columns=['Education'], drop_first=True)\n\n\n\n\nprint(\"--- 方法一: 特征选择 ---\")\nprint(\"首先，我们训练一个基础模型来获取特征重要性...\")\n\n# 训练一个临时的随机森林模型来获取特征重要性排序\ntemp_rf = RandomForestClassifier(n_estimators=500, max_depth=5, random_state=42)\ntemp_rf.fit(X, y)\n\n# 创建一个包含特征名称和其重要性分数的 DataFrame\nfeature_importances = pd.DataFrame({\n    'feature': X.columns,\n    'importance': temp_rf.feature_importances_\n}).sort_values('importance', ascending=False)\n\n# 打印最重要的特征\nprint(\"\\n根据基础模型得出的特征重要性排序:\")\nprint(feature_importances)\n\n--- 方法一: 特征选择 ---\n首先，我们训练一个基础模型来获取特征重要性...\n\n根据基础模型得出的特征重要性排序:\n                         feature  importance\n12                    SCL90R_OCS    0.303654\n9                 CGI_Depressive    0.153773\n11           SCL90R_Somatization    0.127664\n14               PSP_Total_Score    0.098338\n8                   CGI_Negative    0.057625\n3            Duration_of_Illness    0.050011\n7                   CGI_Positive    0.049062\n10                 CGI_Cognitive    0.048823\n0                            Age    0.040797\n13       DIEPSS_Overall_Severity    0.026666\n4                        Obesity    0.009579\n6                       Diabetes    0.007630\n2            Occupation_Employed    0.006329\n15         Education_High school    0.006138\n5                   Hypertension    0.004924\n1                     Sex_Female    0.004714\n16  Education_Less than 12 years    0.004275\n\n\n\n📊 删除建议（按重要性断层）\n我打算删除的变量，其重要性如下：\n\n\n\n特征名称\n重要性\n\n\n\n\nDIEPSS_Overall_Severity\n0.0267\n\n\nObesity\n0.0096\n\n\nDiabetes\n0.0076\n\n\nOccupation_Employed\n0.0063\n\n\nEducation_High school\n0.0061\n\n\nHypertension\n0.0049\n\n\nSex_Female\n0.0047\n\n\nEducation_Less than 12 years\n0.0043\n\n\n\n这些变量都和 Age 的重要性（0.0408）断层，说明它们可能对模型预测贡献非常有限。\n但是考虑到DIEPSS_Oerall_Severity对于实际意义的确有影响，且和其后面的也断层了，所以我打算保留\n特征选择不仅是“数学问题”，也是“领域知识问题”。\n所以我们已经选出了10个贡献比较大的特征\n\n# 选择最重要的前 10 个特征\nN_TOP_FEATURES = 10\ntop_features = feature_importances['feature'].head(N_TOP_FEATURES).tolist()\nprint(f\"\\n我们将选择最重要的 {N_TOP_FEATURES} 个特征进行建模: {top_features}\")\n\n# 创建只包含最重要特征的新数据集\nX_selected = X[top_features]\n\n# --- 3. 在筛选后的特征上重新评估模型 ---\n# Re-evaluate the model on selected features\n\nprint(\"\\n使用筛选出的特征进行5折交叉验证...\")\n# 使用与之前相同的模型配置\nmodel_on_selected_features = RandomForestClassifier(n_estimators=500, max_depth=5, random_state=42)\ncv_scores_selected = cross_val_score(model_on_selected_features, X_selected, y, cv=5, scoring='roc_auc')\n\n# 打印结果\nmean_auc_selected = np.mean(cv_scores_selected)\nprint(f\"\\n使用最重要的 {N_TOP_FEATURES} 个特征, 模型的平均 AUC 分数是: {mean_auc_selected:.4f}\")\nprint(f\"原始模型的平均 AUC 分数是: 0.7980 (作为对比)\")\n\n\n我们将选择最重要的 10 个特征进行建模: ['SCL90R_OCS', 'CGI_Depressive', 'SCL90R_Somatization', 'PSP_Total_Score', 'CGI_Negative', 'Duration_of_Illness', 'CGI_Positive', 'CGI_Cognitive', 'Age', 'DIEPSS_Overall_Severity']\n\n使用筛选出的特征进行5折交叉验证...\n\n使用最重要的 10 个特征, 模型的平均 AUC 分数是: 0.7991\n原始模型的平均 AUC 分数是: 0.7980 (作为对比)\n\n\n为了加以区别，我们分别复制代码，选取9个和11个特征进行训练看看有什么区别，即剔除DIEPSS_Overall_Severity 0.026666 和加上不太重要的Obesity 0.009579的变量\n\n# 选择最重要的前 9 个特征\n#剔除DIEPSS_Overall_Severity 0.026666\nN_TOP_FEATURES = 9\ntop_features = feature_importances['feature'].head(N_TOP_FEATURES).tolist()\nprint(f\"\\n我们将选择最重要的 {N_TOP_FEATURES} 个特征进行建模: {top_features}\")\n\n# 创建只包含最重要特征的新数据集\nX_selected = X[top_features]\n\n# --- 3. 在筛选后的特征上重新评估模型 ---\n# Re-evaluate the model on selected features\n\nprint(\"\\n使用筛选出的特征进行5折交叉验证...\")\n# 使用与之前相同的模型配置\nmodel_on_selected_features = RandomForestClassifier(n_estimators=500, max_depth=5, random_state=42)\ncv_scores_selected = cross_val_score(model_on_selected_features, X_selected, y, cv=5, scoring='roc_auc' )\n\n# 打印结果\nmean_auc_selected = np.mean(cv_scores_selected)\nprint(f\"\\n使用最重要的 {N_TOP_FEATURES} 个特征, 模型的平均 AUC 分数是: {mean_auc_selected:.4f}\")\nprint(f\"原始模型的平均 AUC 分数是: 0.7980 (作为对比)\")\n\n\n我们将选择最重要的 9 个特征进行建模: ['SCL90R_OCS', 'CGI_Depressive', 'SCL90R_Somatization', 'PSP_Total_Score', 'CGI_Negative', 'Duration_of_Illness', 'CGI_Positive', 'CGI_Cognitive', 'Age']\n\n使用筛选出的特征进行5折交叉验证...\n\n使用最重要的 9 个特征, 模型的平均 AUC 分数是: 0.7975\n原始模型的平均 AUC 分数是: 0.7980 (作为对比)\n\n\n\n# 选择最重要的前 11 个特征\n# 加上不太重要的Obesity 0.009579的变量\nN_TOP_FEATURES = 11\ntop_features = feature_importances['feature'].head(N_TOP_FEATURES).tolist()\nprint(f\"\\n我们将选择最重要的 {N_TOP_FEATURES} 个特征进行建模: {top_features}\")\n\n# 创建只包含最重要特征的新数据集\nX_selected = X[top_features]\n\n# --- 3. 在筛选后的特征上重新评估模型 ---\n# Re-evaluate the model on selected features\n\nprint(\"\\n使用筛选出的特征进行5折交叉验证...\")\n# 使用与之前相同的模型配置\nmodel_on_selected_features = RandomForestClassifier(n_estimators=500, max_depth=5, random_state=42)\ncv_scores_selected = cross_val_score(model_on_selected_features, X_selected, y, cv=5, scoring='roc_auc')\n\n# 打印结果\nmean_auc_selected = np.mean(cv_scores_selected)\nprint(f\"\\n使用最重要的 {N_TOP_FEATURES} 个特征, 模型的平均 AUC 分数是: {mean_auc_selected:.4f}\")\nprint(f\"原始模型的平均 AUC 分数是: 0.7980 (作为对比)\")\n\n\n我们将选择最重要的 11 个特征进行建模: ['SCL90R_OCS', 'CGI_Depressive', 'SCL90R_Somatization', 'PSP_Total_Score', 'CGI_Negative', 'Duration_of_Illness', 'CGI_Positive', 'CGI_Cognitive', 'Age', 'DIEPSS_Overall_Severity', 'Obesity']\n\n使用筛选出的特征进行5折交叉验证...\n\n使用最重要的 11 个特征, 模型的平均 AUC 分数是: 0.7974\n原始模型的平均 AUC 分数是: 0.7980 (作为对比)\n\n\n\n\n🎯 教学总结：特征选择的实证价值\n\n\n\n特征数量\n平均 AUC 分数\n相对表现\n\n\n\n\n原始模型（全部特征）\n0.7980\n基准\n\n\n最重要的 10 个特征\n0.7991\n✅ 最佳表现\n\n\n最重要的 9 个特征（剔除 DIEPSS）\n0.7975\n略低\n\n\n最重要的 11 个特征（加入 Obesity）\n0.7974\n略低\n\n\n\n\n\n🧠 要点一：不是特征越多越好\n\n增加变量并不一定提升模型性能，反而可能引入噪声或冗余信息。\n\n\n加入 Obesity（0.009579） 后，模型性能略微下降；\n删除 DIEPSS（0.026666） 后，模型也略微下降；\n说明这两个变量虽然贡献不大，但也不是完全无效，尤其 DIEPSS 可能有一定临床解释价值。\n\n\n\n🧠 要点二：找到“最佳变量组合”才是关键\n\n10个变量组合的模型表现最好，说明它在信息量与简洁性之间达到了平衡。\n\n\n这个组合既保留了高贡献变量，又避免了低贡献变量的干扰；\n模型更快、更稳定、更易解释；。\n\n\n\n🧠 要点三：进行“变量敏感性分析”\n\n逐个添加或剔除变量，观察模型性能变化；\n结合领域知识判断是否保留某些临床重要但模型贡献较低的变量；\n用图表展示 AUC 随变量数量变化的趋势，强化“变量选择是策略性决策”的理念。\n\n\n\n\n超参数调优🎃\n例如，假设 n_estimators=500，max_depth=5，但这些设置可能并非我们特定“赛道”（数据集）的最佳设置。通过系统地尝试这些设置的不同组合，我们可以找到能够实现最佳性能的配置 #### 先是网格搜索\n手动定义一个固定的 param_grid 然后进行“暴力”搜索（Brute-force Search），是超参数调优最基础的方法\n\n# 导入 GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\nprint(\"\\n\\n--- 方法二: 超参数调优 (这可能需要几分钟) ---\")\n\n# --- 1. 定义要搜索的超参数网格 ---\n# Define the parameter grid to search\n\n# 这是一个我们要测试的超参数组合的 \"菜单\"\nparam_grid = {\n    'n_estimators': [200, 400, 600],       # 树的数量\n    'max_depth': [4, 5, 6, 7],             # 树的最大深度\n    'min_samples_leaf': [1, 3, 5],         # 一个叶节点上最少的样本数\n    'max_features': ['sqrt', 'log2']       # 每次分裂时考虑的最大特征数\n}\n\n# --- 2. 设置并运行 GridSearchCV ---\n# Setup and run GridSearchCV\n\n# 初始化我们的基础模型\nrf_base = RandomForestClassifier(random_state=42)\n\n# 设置 GridSearchCV\n# estimator: 我们要调优的模型\n# param_grid: 我们定义的参数网格\n# cv=5: 使用5折交叉验证来评估每一种参数组合\n# scoring='roc_auc': 我们的评估指标\n\ngrid_search = GridSearchCV(estimator=rf_base, param_grid=param_grid, cv=5, scoring='roc_auc' , verbose=1)\n\n# 在完整数据集上运行网格搜索\ngrid_search.fit(X, y)\n\n# --- 3. 打印最佳结果 ---\n# Print the best results\n\nprint(\"\\n超参数调优完成!\")\n\n# 打印找到的最佳参数组合\nprint(f\"找到的最佳超参数组合 (Best Parameters): \\n{grid_search.best_params_}\")\n\n# 打印使用最佳参数时，交叉验证得到的最佳AUC分数\nprint(f\"\\n使用最佳参数时，模型的最佳平均 AUC 分数是: {grid_search.best_score_:.4f}\")\n\n\n\n--- 方法二: 超参数调优 (这可能需要几分钟) ---\nFitting 5 folds for each of 72 candidates, totalling 360 fits\n\n超参数调优完成!\n找到的最佳超参数组合 (Best Parameters): \n{'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'n_estimators': 200}\n\n使用最佳参数时，模型的最佳平均 AUC 分数是: 0.7997\n\n\nd:\\pythontext\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning:\n\ninvalid value encountered in cast\n\n\n\n使用最佳参数时，模型的最佳平均 AUC 分数是: 0.7997 正如我说，网格搜索是超参数最基础的方法，但它有两个主要缺点：\n\n维度诅咒 (Curse of Dimensionality): 当参数和你想尝试的值变多时，组合的数量会呈指数级增长，导致计算成本极高，非常耗时。\n效率低下: 它会花费大量时间去尝试那些明显不佳的参数组合。\n\n接下来我会用一个更加爽的方法，即贝叶斯优化\n这是目前最先进和最高效的调优方法之一，它完美地体现了“迭代选择最好”的思想。\n核心思想：\n贝叶斯优化会建立一个关于“哪个参数组合可能得到更高分数”的概率模型。\n\n它首先尝试几个随机的参数组合。\n然后，它根据已有的结果，更新内部的概率模型，预测出“最有希望”提升分数的下一个参数组合。\n它会去尝试这个“最有希望”的组合，然后再次更新模型。\n这个过程会不断迭代，智能地将搜索资源集中在参数空间中最有潜力的区域。\n\n打个比方： 这就像在一个大雾弥漫的山区里找最高峰。你不会在地毯式搜索每一寸土地（网格搜索），而是根据你当前所在位置的高度和坡度，来判断下一步往哪个方向走最有可能登得更高。\n🥸不过需要注意的是，由于是随机组合，每次远行代码得到的最好的结果都不一样，就好像在乌泱泱的大海找最大的船，有的时候你不一定能找到山东舰航母，可能只会找到护卫舰\n\n# 导入 optuna 库\nimport optuna\nfrom sklearn.model_selection import cross_val_score\n\nprint(\"\\n\\n--- 最智能的调优方法二: 贝叶斯优化 (Bayesian Optimization with Optuna) ---\")\nprint(\"Optuna 会根据历史结果，智能地选择下一次要尝试的超参数...\")\n\n# --- 1. 定义一个 \"目标函数\" (Objective Function) ---\n# 这个函数告诉 Optuna 如何评估一组超参数的好坏\ndef objective(trial):\n    # 'trial' 对象用于建议超参数的值\n    # Optuna 会智能地决定建议值的范围和具体值\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 200, 800), # 建议一个在 [200, 800] 范围内的整数\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n    }\n    \n    # 使用这组参数创建并评估模型\n    model = RandomForestClassifier(random_state=42, **params)\n    \n    # 使用交叉验证来获得稳健的分数\n    score = cross_val_score(model, X, y, cv=5, scoring='roc_auc', ).mean()\n    \n    # Optuna 的目标是最大化这个返回值\n    return score\n\n# --- 2. 创建一个 \"研究\" (Study) 并开始优化 ---\n# direction='maximize': 告诉 Optuna 我们的目标是最大化 objective 函数的返回值\nstudy = optuna.create_study(direction='maximize', study_name='RF_Optimization')\n\n# n_trials=100: 运行100次迭代。Optuna 会在这100次中智能地寻找最优解。\nstudy.optimize(objective, n_trials=100)\n\n# --- 3. 打印最佳结果 ---\nprint(\"\\n贝叶斯优化完成!\")\nprint(f\"迭代次数 (Number of finished trials): {len(study.trials)}\")\nprint(f\"找到的最佳超参数组合 (Best Parameters): \\n{study.best_params}\")\nprint(f\"\\n使用最佳参数时，模型的最佳平均 AUC 分数是: {study.best_value:.4f}\")\n\n[I 2025-09-12 12:06:24,092] A new study created in memory with name: RF_Optimization\n\n\n\n\n--- 最智能的调优方法二: 贝叶斯优化 (Bayesian Optimization with Optuna) ---\nOptuna 会根据历史结果，智能地选择下一次要尝试的超参数...\n\n\n[I 2025-09-12 12:06:26,062] Trial 0 finished with value: 0.7975024428376002 and parameters: {'n_estimators': 662, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.7975024428376002.\n[I 2025-09-12 12:06:28,401] Trial 1 finished with value: 0.7987354124860382 and parameters: {'n_estimators': 758, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 1 with value: 0.7987354124860382.\n[I 2025-09-12 12:06:30,359] Trial 2 finished with value: 0.7962259708194112 and parameters: {'n_estimators': 547, 'max_depth': 6, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 1 with value: 0.7987354124860382.\n[I 2025-09-12 12:06:32,669] Trial 3 finished with value: 0.7951182333398477 and parameters: {'n_estimators': 588, 'max_depth': 9, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 1 with value: 0.7987354124860382.\n[I 2025-09-12 12:06:34,517] Trial 4 finished with value: 0.795039522331905 and parameters: {'n_estimators': 507, 'max_depth': 10, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 1 with value: 0.7987354124860382.\n[I 2025-09-12 12:06:35,368] Trial 5 finished with value: 0.7947395657071677 and parameters: {'n_estimators': 226, 'max_depth': 8, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 1 with value: 0.7987354124860382.\n[I 2025-09-12 12:06:38,438] Trial 6 finished with value: 0.7902455071648625 and parameters: {'n_estimators': 786, 'max_depth': 9, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7987354124860382.\n[I 2025-09-12 12:06:40,595] Trial 7 finished with value: 0.7979688301866407 and parameters: {'n_estimators': 642, 'max_depth': 5, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 1 with value: 0.7987354124860382.\n[I 2025-09-12 12:06:41,723] Trial 8 finished with value: 0.7974651369339568 and parameters: {'n_estimators': 334, 'max_depth': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7987354124860382.\n[I 2025-09-12 12:06:43,010] Trial 9 finished with value: 0.796392687534458 and parameters: {'n_estimators': 358, 'max_depth': 6, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7987354124860382.\n[I 2025-09-12 12:06:45,395] Trial 10 finished with value: 0.7988974262421928 and parameters: {'n_estimators': 794, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 10 with value: 0.7988974262421928.\n[I 2025-09-12 12:06:47,759] Trial 11 finished with value: 0.7989554982514845 and parameters: {'n_estimators': 780, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 11 with value: 0.7989554982514845.\n[I 2025-09-12 12:06:50,104] Trial 12 finished with value: 0.7962377123282666 and parameters: {'n_estimators': 718, 'max_depth': 4, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 11 with value: 0.7989554982514845.\n[I 2025-09-12 12:06:52,505] Trial 13 finished with value: 0.7991797308827104 and parameters: {'n_estimators': 784, 'max_depth': 3, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:06:54,654] Trial 14 finished with value: 0.7985183455117725 and parameters: {'n_estimators': 676, 'max_depth': 4, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:06:56,302] Trial 15 finished with value: 0.7961692652103383 and parameters: {'n_estimators': 433, 'max_depth': 7, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:06:58,239] Trial 16 finished with value: 0.7980679736443641 and parameters: {'n_estimators': 594, 'max_depth': 4, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:00,746] Trial 17 finished with value: 0.7981055178736269 and parameters: {'n_estimators': 733, 'max_depth': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:02,854] Trial 18 finished with value: 0.7987934368302058 and parameters: {'n_estimators': 708, 'max_depth': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:04,519] Trial 19 finished with value: 0.7927481803838949 and parameters: {'n_estimators': 446, 'max_depth': 7, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:06,439] Trial 20 finished with value: 0.7965123746605846 and parameters: {'n_estimators': 602, 'max_depth': 4, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:08,722] Trial 21 finished with value: 0.7984538545991284 and parameters: {'n_estimators': 772, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:11,054] Trial 22 finished with value: 0.7986725898527307 and parameters: {'n_estimators': 796, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:13,320] Trial 23 finished with value: 0.7970167670017525 and parameters: {'n_estimators': 699, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:16,033] Trial 24 finished with value: 0.7969568360526286 and parameters: {'n_estimators': 796, 'max_depth': 5, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:17,950] Trial 25 finished with value: 0.797897348389157 and parameters: {'n_estimators': 647, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:20,310] Trial 26 finished with value: 0.7973571436515616 and parameters: {'n_estimators': 741, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:22,519] Trial 27 finished with value: 0.7985077797426401 and parameters: {'n_estimators': 750, 'max_depth': 3, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:24,958] Trial 28 finished with value: 0.7962730480734551 and parameters: {'n_estimators': 696, 'max_depth': 6, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:26,896] Trial 29 finished with value: 0.7975611186051278 and parameters: {'n_estimators': 663, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:29,090] Trial 30 finished with value: 0.7983943844128689 and parameters: {'n_estimators': 639, 'max_depth': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:31,244] Trial 31 finished with value: 0.7985672499288995 and parameters: {'n_estimators': 730, 'max_depth': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:33,451] Trial 32 finished with value: 0.7985686481058675 and parameters: {'n_estimators': 750, 'max_depth': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:36,126] Trial 33 finished with value: 0.7973512808413212 and parameters: {'n_estimators': 796, 'max_depth': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:38,171] Trial 34 finished with value: 0.7982323229915902 and parameters: {'n_estimators': 701, 'max_depth': 3, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:40,563] Trial 35 finished with value: 0.7973017250008342 and parameters: {'n_estimators': 758, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:42,078] Trial 36 finished with value: 0.7977246258684982 and parameters: {'n_estimators': 516, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:43,696] Trial 37 finished with value: 0.7977758023231981 and parameters: {'n_estimators': 550, 'max_depth': 3, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:46,833] Trial 38 finished with value: 0.7908430848232657 and parameters: {'n_estimators': 769, 'max_depth': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:48,996] Trial 39 finished with value: 0.7970188166020804 and parameters: {'n_estimators': 683, 'max_depth': 4, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:50,032] Trial 40 finished with value: 0.7949377414039922 and parameters: {'n_estimators': 259, 'max_depth': 9, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:52,125] Trial 41 finished with value: 0.7978447260923656 and parameters: {'n_estimators': 714, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:54,374] Trial 42 finished with value: 0.7986793424119506 and parameters: {'n_estimators': 766, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:56,335] Trial 43 finished with value: 0.7959593162279092 and parameters: {'n_estimators': 617, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:59,169] Trial 44 finished with value: 0.7920274360453199 and parameters: {'n_estimators': 723, 'max_depth': 8, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:01,582] Trial 45 finished with value: 0.7984456561978167 and parameters: {'n_estimators': 800, 'max_depth': 3, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:04,281] Trial 46 finished with value: 0.798224410581022 and parameters: {'n_estimators': 770, 'max_depth': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:06,043] Trial 47 finished with value: 0.797908803907269 and parameters: {'n_estimators': 557, 'max_depth': 4, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:08,444] Trial 48 finished with value: 0.7954266108031415 and parameters: {'n_estimators': 678, 'max_depth': 8, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:10,962] Trial 49 finished with value: 0.7985558897410353 and parameters: {'n_estimators': 739, 'max_depth': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:13,510] Trial 50 finished with value: 0.7984145944254049 and parameters: {'n_estimators': 781, 'max_depth': 4, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:15,836] Trial 51 finished with value: 0.7986233200029871 and parameters: {'n_estimators': 767, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:17,925] Trial 52 finished with value: 0.7979007485013291 and parameters: {'n_estimators': 713, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:20,189] Trial 53 finished with value: 0.7986760376300264 and parameters: {'n_estimators': 774, 'max_depth': 3, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:22,379] Trial 54 finished with value: 0.7982296696330262 and parameters: {'n_estimators': 746, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:23,807] Trial 55 finished with value: 0.7959633200983173 and parameters: {'n_estimators': 448, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:26,099] Trial 56 finished with value: 0.7989554982514845 and parameters: {'n_estimators': 780, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:27,258] Trial 57 finished with value: 0.7975624214518479 and parameters: {'n_estimators': 381, 'max_depth': 3, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:29,353] Trial 58 finished with value: 0.7961802917423351 and parameters: {'n_estimators': 658, 'max_depth': 4, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:31,678] Trial 59 finished with value: 0.797187630582579 and parameters: {'n_estimators': 735, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:34,000] Trial 60 finished with value: 0.798616567443767 and parameters: {'n_estimators': 790, 'max_depth': 3, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:36,296] Trial 61 finished with value: 0.7987354124860382 and parameters: {'n_estimators': 758, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:38,348] Trial 62 finished with value: 0.7978979998125172 and parameters: {'n_estimators': 700, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:41,160] Trial 63 finished with value: 0.7944944398632965 and parameters: {'n_estimators': 754, 'max_depth': 7, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:43,446] Trial 64 finished with value: 0.7989554982514845 and parameters: {'n_estimators': 780, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:45,962] Trial 65 finished with value: 0.7978601854808856 and parameters: {'n_estimators': 800, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:48,085] Trial 66 finished with value: 0.7982924446012097 and parameters: {'n_estimators': 723, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:50,377] Trial 67 finished with value: 0.7991790317942264 and parameters: {'n_estimators': 783, 'max_depth': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:52,708] Trial 68 finished with value: 0.7991797308827104 and parameters: {'n_estimators': 784, 'max_depth': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:54,965] Trial 69 finished with value: 0.7989533533209088 and parameters: {'n_estimators': 779, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:57,231] Trial 70 finished with value: 0.7991207691244393 and parameters: {'n_estimators': 783, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:59,495] Trial 71 finished with value: 0.7988945822231333 and parameters: {'n_estimators': 781, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:09:01,790] Trial 72 finished with value: 0.7991228187247673 and parameters: {'n_estimators': 786, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:09:04,018] Trial 73 finished with value: 0.7979473649925086 and parameters: {'n_estimators': 744, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:09:06,283] Trial 74 finished with value: 0.7988433104381855 and parameters: {'n_estimators': 777, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:09:08,743] Trial 75 finished with value: 0.7968341619120706 and parameters: {'n_estimators': 728, 'max_depth': 6, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:09:11,133] Trial 76 finished with value: 0.7989633153318049 and parameters: {'n_estimators': 757, 'max_depth': 4, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:09:13,504] Trial 77 finished with value: 0.7990173358055644 and parameters: {'n_estimators': 754, 'max_depth': 4, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:09:16,033] Trial 78 finished with value: 0.7980208010600724 and parameters: {'n_estimators': 757, 'max_depth': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:09:18,217] Trial 79 finished with value: 0.7984650241264969 and parameters: {'n_estimators': 692, 'max_depth': 4, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:09:19,009] Trial 80 finished with value: 0.7998832839999301 and parameters: {'n_estimators': 249, 'max_depth': 4, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 80 with value: 0.7998832839999301.\n[I 2025-09-12 12:09:20,085] Trial 81 finished with value: 0.7987002832897196 and parameters: {'n_estimators': 340, 'max_depth': 4, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 80 with value: 0.7998832839999301.\n[I 2025-09-12 12:09:20,780] Trial 82 finished with value: 0.7999420074325816 and parameters: {'n_estimators': 217, 'max_depth': 4, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 82 with value: 0.7999420074325816.\n[I 2025-09-12 12:09:21,444] Trial 83 finished with value: 0.79937963841237 and parameters: {'n_estimators': 205, 'max_depth': 4, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 82 with value: 0.7999420074325816.\n[I 2025-09-12 12:09:22,218] Trial 84 finished with value: 0.8005951467370839 and parameters: {'n_estimators': 230, 'max_depth': 5, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 84 with value: 0.8005951467370839.\n[I 2025-09-12 12:09:22,923] Trial 85 finished with value: 0.8010832852710001 and parameters: {'n_estimators': 209, 'max_depth': 5, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 85 with value: 0.8010832852710001.\n[I 2025-09-12 12:09:23,606] Trial 86 finished with value: 0.8007435600445509 and parameters: {'n_estimators': 205, 'max_depth': 5, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 85 with value: 0.8010832852710001.\n[I 2025-09-12 12:09:24,289] Trial 87 finished with value: 0.8028189630928946 and parameters: {'n_estimators': 203, 'max_depth': 5, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 87 with value: 0.8028189630928946.\n[I 2025-09-12 12:09:25,005] Trial 88 finished with value: 0.8034329375539212 and parameters: {'n_estimators': 212, 'max_depth': 5, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 88 with value: 0.8034329375539212.\n[I 2025-09-12 12:09:25,691] Trial 89 finished with value: 0.8029850283845812 and parameters: {'n_estimators': 200, 'max_depth': 5, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 88 with value: 0.8034329375539212.\n[I 2025-09-12 12:09:26,379] Trial 90 finished with value: 0.8031572424772518 and parameters: {'n_estimators': 202, 'max_depth': 5, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 88 with value: 0.8034329375539212.\n[I 2025-09-12 12:09:27,084] Trial 91 finished with value: 0.8037109999984111 and parameters: {'n_estimators': 209, 'max_depth': 5, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 91 with value: 0.8037109999984111.\n[I 2025-09-12 12:09:27,963] Trial 92 finished with value: 0.7980920445319364 and parameters: {'n_estimators': 254, 'max_depth': 6, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 91 with value: 0.8037109999984111.\n[I 2025-09-12 12:09:28,780] Trial 93 finished with value: 0.8030499482833406 and parameters: {'n_estimators': 244, 'max_depth': 5, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 91 with value: 0.8037109999984111.\n[I 2025-09-12 12:09:29,726] Trial 94 finished with value: 0.8021223373070159 and parameters: {'n_estimators': 281, 'max_depth': 5, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 91 with value: 0.8037109999984111.\n[I 2025-09-12 12:09:30,681] Trial 95 finished with value: 0.8022945037345626 and parameters: {'n_estimators': 284, 'max_depth': 5, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 91 with value: 0.8037109999984111.\n[I 2025-09-12 12:09:31,655] Trial 96 finished with value: 0.8024038951939255 and parameters: {'n_estimators': 287, 'max_depth': 5, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 91 with value: 0.8037109999984111.\n[I 2025-09-12 12:09:32,636] Trial 97 finished with value: 0.8022310773430188 and parameters: {'n_estimators': 291, 'max_depth': 5, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 91 with value: 0.8037109999984111.\n[I 2025-09-12 12:09:33,649] Trial 98 finished with value: 0.7965552573837249 and parameters: {'n_estimators': 291, 'max_depth': 6, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 91 with value: 0.8037109999984111.\n[I 2025-09-12 12:09:34,614] Trial 99 finished with value: 0.802574059686268 and parameters: {'n_estimators': 286, 'max_depth': 5, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 91 with value: 0.8037109999984111.\n\n\n\n贝叶斯优化完成!\n迭代次数 (Number of finished trials): 100\n找到的最佳超参数组合 (Best Parameters): \n{'n_estimators': 209, 'max_depth': 5, 'min_samples_leaf': 8, 'max_features': 'sqrt'}\n\n使用最佳参数时，模型的最佳平均 AUC 分数是: 0.8037\n\n\n使用最佳参数时，模型的最佳平均 AUC 分数是: 0.8042(差不多远行了好几次的最好数据),这是迭代只有100次，但是也可以多迭代，设置到200，500等等，只要你的计算机能抗住。\n\n\n最终优化：结合特征选择与贝叶斯调优🤤\n目标： 在我们已经验证过的“最佳特征集”（最重要的10个特征）上，使用贝叶斯优化（Optuna）来寻找最优的超参数组合，从而获得理论上性能最好的模型\n\n# 导入所有需要的库\nimport pandas as pd\nimport numpy as np\nimport optuna\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\n\n# --- 1. 最终步骤一：确定并筛选出最佳特征集 (Top 10) ---\n# The first step of our final process: identify and select the best feature set.\n\n# !!! 重要提示: 请将下面的路径修改为您保存 CSV 文件的实际位置 !!!\nfile_path = \"./synthetic_schizophrenia_data.csv\"\ndf = pd.read_csv(file_path)\n\ndf['High_SWB'] = (df['SWN_Total_Score'] &gt;= 80).astype(int)\nX = df.drop(['SWN_Total_Score', 'High_SWB'], axis=1)\ny = df['High_SWB']\nX = pd.get_dummies(X, columns=['Education'], drop_first=True)\n\n\n# 训练一个临时模型来获取特征重要性\ntemp_rf = RandomForestClassifier(n_estimators=500, max_depth=5, random_state=42, )\ntemp_rf.fit(X, y)\n\n# 获取最重要的10个特征的名称\nN_TOP_FEATURES = 10\nfeature_importances = pd.DataFrame({\n    'feature': X.columns,\n    'importance': temp_rf.feature_importances_\n}).sort_values('importance', ascending=False)\ntop_10_features = feature_importances['feature'].head(N_TOP_FEATURES).tolist()\n\n# 创建只包含这10个特征的新数据集\nX_top10 = X[top_10_features]\n\nprint(\"--- 最终优化流程 ---\")\nprint(f\"已成功筛选出最重要的 {N_TOP_FEATURES} 个特征用于最终调优:\")\nprint(top_10_features)\n\n\n# --- 2. 最终步骤二：在最佳特征集上进行贝叶斯优化 ---\n# The second step: run Bayesian Optimization on this optimal feature set.\n\nprint(\"\\n现在，我们只在这10个特征上进行贝叶斯优化 (Optuna)...\")\n\n# 定义目标函数，与之前类似，但这次使用的是筛选后的数据\ndef objective_top10(trial):\n    # 定义超参数搜索空间\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 200, 800),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n    }\n    \n    model = RandomForestClassifier(random_state=42, **params)\n    \n    # ！！！ 关键不同点 ！！！\n    # 注意：我们在这里使用 X_top10，而不是完整的 X\n    # CRITICAL DIFFERENCE: We are using X_top10 here, not the full X.\n    score = cross_val_score(model, X_top10, y, cv=5, scoring='roc_auc').mean()\n    \n    return score\n\n# 创建一个新的 \"研究\" (Study)\nstudy_top10 = optuna.create_study(direction='maximize', study_name='RF_Top10_Optimization')\n\n# 运行优化，n_trials 可以根据需要调整\nstudy_top10.optimize(objective_top10, n_trials=100)\n\n# --- 3. 打印最终优化结果 ---\n# Print the final optimization results\n\nbest_params_top10 = study_top10.best_params\nbest_score_top10 = study_top10.best_value\n\nprint(\"\\n在Top 10特征集上的贝叶斯优化完成!\")\nprint(f\"找到的最佳超参数组合 (Best Parameters): \\n{best_params_top10}\")\nprint(f\"\\n最终模型的最佳平均 AUC 分数是: {best_score_top10:.4f}\")\n\n# --- 4. 训练最终的、最优化的模型 ---\n# Train the final, most optimized model\n\nprint(\"\\n正在使用找到的最佳特征和最佳超参数来训练最终模型...\")\nfinal_model = RandomForestClassifier(random_state=42,  **best_params_top10)\nfinal_model.fit(X_top10, y)\n\nprint(\"最终模型训练完成！这个模型是我们整个分析过程的最终成果。\")\n\n[I 2025-09-12 12:09:35,006] A new study created in memory with name: RF_Top10_Optimization\n\n\n--- 最终优化流程 ---\n已成功筛选出最重要的 10 个特征用于最终调优:\n['SCL90R_OCS', 'CGI_Depressive', 'SCL90R_Somatization', 'PSP_Total_Score', 'CGI_Negative', 'Duration_of_Illness', 'CGI_Positive', 'CGI_Cognitive', 'Age', 'DIEPSS_Overall_Severity']\n\n现在，我们只在这10个特征上进行贝叶斯优化 (Optuna)...\n\n\n[I 2025-09-12 12:09:37,678] Trial 0 finished with value: 0.7974902882310043 and parameters: {'n_estimators': 709, 'max_depth': 7, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 0 with value: 0.7974902882310043.\n[I 2025-09-12 12:09:38,587] Trial 1 finished with value: 0.8014817339301008 and parameters: {'n_estimators': 271, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.8014817339301008.\n[I 2025-09-12 12:09:39,567] Trial 2 finished with value: 0.8006315628917478 and parameters: {'n_estimators': 293, 'max_depth': 4, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 1 with value: 0.8014817339301008.\n[I 2025-09-12 12:09:40,647] Trial 3 finished with value: 0.8014801450926372 and parameters: {'n_estimators': 322, 'max_depth': 4, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 1 with value: 0.8014817339301008.\n[I 2025-09-12 12:09:41,512] Trial 4 finished with value: 0.7991735820817267 and parameters: {'n_estimators': 222, 'max_depth': 9, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 1 with value: 0.8014817339301008.\n[I 2025-09-12 12:09:44,209] Trial 5 finished with value: 0.7990501135224367 and parameters: {'n_estimators': 748, 'max_depth': 7, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 1 with value: 0.8014817339301008.\n[I 2025-09-12 12:09:45,970] Trial 6 finished with value: 0.7985547616664362 and parameters: {'n_estimators': 502, 'max_depth': 5, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 1 with value: 0.8014817339301008.\n[I 2025-09-12 12:09:46,889] Trial 7 finished with value: 0.7994638309095616 and parameters: {'n_estimators': 258, 'max_depth': 5, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 1 with value: 0.8014817339301008.\n[I 2025-09-12 12:09:48,505] Trial 8 finished with value: 0.799350769235658 and parameters: {'n_estimators': 436, 'max_depth': 7, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.8014817339301008.\n[I 2025-09-12 12:09:51,306] Trial 9 finished with value: 0.7988488554809331 and parameters: {'n_estimators': 781, 'max_depth': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.8014817339301008.\n[I 2025-09-12 12:09:52,952] Trial 10 finished with value: 0.8026295736672434 and parameters: {'n_estimators': 545, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:09:54,757] Trial 11 finished with value: 0.8025121268019404 and parameters: {'n_estimators': 597, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:09:56,509] Trial 12 finished with value: 0.8025114277134563 and parameters: {'n_estimators': 583, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:09:58,411] Trial 13 finished with value: 0.8019518073820567 and parameters: {'n_estimators': 634, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:00,327] Trial 14 finished with value: 0.7980027677548615 and parameters: {'n_estimators': 496, 'max_depth': 10, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:02,236] Trial 15 finished with value: 0.802179201799835 and parameters: {'n_estimators': 613, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:03,651] Trial 16 finished with value: 0.7990156039727292 and parameters: {'n_estimators': 396, 'max_depth': 6, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:05,581] Trial 17 finished with value: 0.7979236913143022 and parameters: {'n_estimators': 545, 'max_depth': 8, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:07,601] Trial 18 finished with value: 0.8022298698265466 and parameters: {'n_estimators': 671, 'max_depth': 3, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:08,972] Trial 19 finished with value: 0.798337599361923 and parameters: {'n_estimators': 388, 'max_depth': 6, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:10,729] Trial 20 finished with value: 0.8015777156012718 and parameters: {'n_estimators': 536, 'max_depth': 4, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:12,487] Trial 21 finished with value: 0.8024547062160087 and parameters: {'n_estimators': 585, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:14,451] Trial 22 finished with value: 0.8019002496063656 and parameters: {'n_estimators': 654, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:16,000] Trial 23 finished with value: 0.8007322951869347 and parameters: {'n_estimators': 479, 'max_depth': 4, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:17,927] Trial 24 finished with value: 0.7993457802860224 and parameters: {'n_estimators': 562, 'max_depth': 5, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:20,111] Trial 25 finished with value: 0.8017842962482782 and parameters: {'n_estimators': 698, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:22,078] Trial 26 finished with value: 0.800559477336028 and parameters: {'n_estimators': 611, 'max_depth': 4, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:23,421] Trial 27 finished with value: 0.8026821959640351 and parameters: {'n_estimators': 443, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 27 with value: 0.8026821959640351.\n[I 2025-09-12 12:10:24,684] Trial 28 finished with value: 0.8006787831411634 and parameters: {'n_estimators': 367, 'max_depth': 5, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 27 with value: 0.8026821959640351.\n[I 2025-09-12 12:10:26,289] Trial 29 finished with value: 0.7973467049894263 and parameters: {'n_estimators': 444, 'max_depth': 6, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 27 with value: 0.8026821959640351.\n[I 2025-09-12 12:10:27,720] Trial 30 finished with value: 0.8015735687354919 and parameters: {'n_estimators': 441, 'max_depth': 4, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 27 with value: 0.8026821959640351.\n[I 2025-09-12 12:10:29,317] Trial 31 finished with value: 0.8027416184851706 and parameters: {'n_estimators': 530, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 31 with value: 0.8027416184851706.\n[I 2025-09-12 12:10:30,882] Trial 32 finished with value: 0.8024012418353615 and parameters: {'n_estimators': 517, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 31 with value: 0.8027416184851706.\n[I 2025-09-12 12:10:32,395] Trial 33 finished with value: 0.8017477370982424 and parameters: {'n_estimators': 465, 'max_depth': 4, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 31 with value: 0.8027416184851706.\n[I 2025-09-12 12:10:34,256] Trial 34 finished with value: 0.8003893605088093 and parameters: {'n_estimators': 573, 'max_depth': 4, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 31 with value: 0.8027416184851706.\n[I 2025-09-12 12:10:35,841] Trial 35 finished with value: 0.8027395688848425 and parameters: {'n_estimators': 526, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 31 with value: 0.8027416184851706.\n[I 2025-09-12 12:10:36,969] Trial 36 finished with value: 0.8030340281319562 and parameters: {'n_estimators': 336, 'max_depth': 4, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 36 with value: 0.8030340281319562.\n[I 2025-09-12 12:10:38,038] Trial 37 finished with value: 0.8030425919658845 and parameters: {'n_estimators': 326, 'max_depth': 4, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 37 with value: 0.8030425919658845.\n[I 2025-09-12 12:10:39,100] Trial 38 finished with value: 0.80309461050444 and parameters: {'n_estimators': 324, 'max_depth': 4, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 38 with value: 0.80309461050444.\n[I 2025-09-12 12:10:40,241] Trial 39 finished with value: 0.799110981885664 and parameters: {'n_estimators': 325, 'max_depth': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 38 with value: 0.80309461050444.\n[I 2025-09-12 12:10:41,059] Trial 40 finished with value: 0.797393972903966 and parameters: {'n_estimators': 214, 'max_depth': 8, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 38 with value: 0.80309461050444.\n[I 2025-09-12 12:10:42,049] Trial 41 finished with value: 0.8029259395193131 and parameters: {'n_estimators': 296, 'max_depth': 4, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 38 with value: 0.80309461050444.\n[I 2025-09-12 12:10:42,992] Trial 42 finished with value: 0.8025842600227839 and parameters: {'n_estimators': 279, 'max_depth': 4, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 38 with value: 0.80309461050444.\n[I 2025-09-12 12:10:44,166] Trial 43 finished with value: 0.7979568503521658 and parameters: {'n_estimators': 314, 'max_depth': 5, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 38 with value: 0.80309461050444.\n[I 2025-09-12 12:10:44,999] Trial 44 finished with value: 0.8026418712692112 and parameters: {'n_estimators': 250, 'max_depth': 4, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 38 with value: 0.80309461050444.\n[I 2025-09-12 12:10:46,195] Trial 45 finished with value: 0.8011576110875435 and parameters: {'n_estimators': 342, 'max_depth': 5, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 38 with value: 0.80309461050444.\n[I 2025-09-12 12:10:47,015] Trial 46 finished with value: 0.8033251825971455 and parameters: {'n_estimators': 244, 'max_depth': 4, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:47,901] Trial 47 finished with value: 0.7975646934894207 and parameters: {'n_estimators': 240, 'max_depth': 5, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:49,013] Trial 48 finished with value: 0.7981127788608353 and parameters: {'n_estimators': 294, 'max_depth': 6, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:50,275] Trial 49 finished with value: 0.8000130761323249 and parameters: {'n_estimators': 358, 'max_depth': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:51,239] Trial 50 finished with value: 0.8010956305380917 and parameters: {'n_estimators': 289, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:51,931] Trial 51 finished with value: 0.8011262315476388 and parameters: {'n_estimators': 206, 'max_depth': 4, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:53,237] Trial 52 finished with value: 0.8019321057975091 and parameters: {'n_estimators': 399, 'max_depth': 4, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:54,273] Trial 53 finished with value: 0.8028808165353493 and parameters: {'n_estimators': 313, 'max_depth': 4, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:55,207] Trial 54 finished with value: 0.79790157469681 and parameters: {'n_estimators': 264, 'max_depth': 5, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:56,410] Trial 55 finished with value: 0.7992486546518778 and parameters: {'n_estimators': 312, 'max_depth': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:57,528] Trial 56 finished with value: 0.7999153785166933 and parameters: {'n_estimators': 336, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:58,413] Trial 57 finished with value: 0.7939838033908969 and parameters: {'n_estimators': 233, 'max_depth': 7, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:59,998] Trial 58 finished with value: 0.7960905065372718 and parameters: {'n_estimators': 415, 'max_depth': 6, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:11:00,978] Trial 59 finished with value: 0.8028706161988335 and parameters: {'n_estimators': 299, 'max_depth': 4, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:11:02,367] Trial 60 finished with value: 0.7986887959948585 and parameters: {'n_estimators': 369, 'max_depth': 8, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:11:03,353] Trial 61 finished with value: 0.8027565217805783 and parameters: {'n_estimators': 300, 'max_depth': 4, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:11:04,225] Trial 62 finished with value: 0.8017607337886942 and parameters: {'n_estimators': 265, 'max_depth': 4, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:11:05,399] Trial 63 finished with value: 0.8020949139723956 and parameters: {'n_estimators': 355, 'max_depth': 4, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:11:06,372] Trial 64 finished with value: 0.8008678071341981 and parameters: {'n_estimators': 278, 'max_depth': 5, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:11:07,358] Trial 65 finished with value: 0.8037308445783304 and parameters: {'n_estimators': 323, 'max_depth': 3, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:08,376] Trial 66 finished with value: 0.8028496117675659 and parameters: {'n_estimators': 334, 'max_depth': 3, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:09,515] Trial 67 finished with value: 0.8031817900160633 and parameters: {'n_estimators': 377, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:10,677] Trial 68 finished with value: 0.8031838396163911 and parameters: {'n_estimators': 383, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:11,888] Trial 69 finished with value: 0.8032432621375266 and parameters: {'n_estimators': 401, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:13,046] Trial 70 finished with value: 0.8030710957099799 and parameters: {'n_estimators': 382, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:14,199] Trial 71 finished with value: 0.802961704250617 and parameters: {'n_estimators': 381, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:15,451] Trial 72 finished with value: 0.8027349135910746 and parameters: {'n_estimators': 413, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:16,687] Trial 73 finished with value: 0.8027929379352422 and parameters: {'n_estimators': 409, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:17,943] Trial 74 finished with value: 0.8034633955680966 and parameters: {'n_estimators': 386, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:19,269] Trial 75 finished with value: 0.8027976885592581 and parameters: {'n_estimators': 425, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:20,425] Trial 76 finished with value: 0.802961704250617 and parameters: {'n_estimators': 381, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:21,493] Trial 77 finished with value: 0.8033472038843898 and parameters: {'n_estimators': 353, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:22,903] Trial 78 finished with value: 0.802961005162133 and parameters: {'n_estimators': 466, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:24,004] Trial 79 finished with value: 0.8034598524605532 and parameters: {'n_estimators': 364, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:25,125] Trial 80 finished with value: 0.8035132691760765 and parameters: {'n_estimators': 370, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:26,199] Trial 81 finished with value: 0.8033472038843898 and parameters: {'n_estimators': 353, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:27,270] Trial 82 finished with value: 0.8035166692882484 and parameters: {'n_estimators': 352, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:28,339] Trial 83 finished with value: 0.8035166692882484 and parameters: {'n_estimators': 352, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:29,415] Trial 84 finished with value: 0.8031216684064437 and parameters: {'n_estimators': 354, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:30,469] Trial 85 finished with value: 0.8032945339224742 and parameters: {'n_estimators': 347, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:31,578] Trial 86 finished with value: 0.8037974804215503 and parameters: {'n_estimators': 363, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 86 with value: 0.8037974804215503.\n[I 2025-09-12 12:11:32,688] Trial 87 finished with value: 0.8037394560773828 and parameters: {'n_estimators': 367, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 86 with value: 0.8037974804215503.\n[I 2025-09-12 12:11:33,800] Trial 88 finished with value: 0.8032297093539629 and parameters: {'n_estimators': 367, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 86 with value: 0.8037974804215503.\n[I 2025-09-12 12:11:35,173] Trial 89 finished with value: 0.8025173381888207 and parameters: {'n_estimators': 455, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 86 with value: 0.8037974804215503.\n[I 2025-09-12 12:11:36,467] Trial 90 finished with value: 0.8026294306718716 and parameters: {'n_estimators': 428, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 86 with value: 0.8037974804215503.\n[I 2025-09-12 12:11:37,563] Trial 91 finished with value: 0.8035200217352966 and parameters: {'n_estimators': 362, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 86 with value: 0.8037974804215503.\n[I 2025-09-12 12:11:38,750] Trial 92 finished with value: 0.8026253791363398 and parameters: {'n_estimators': 393, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 86 with value: 0.8037974804215503.\n[I 2025-09-12 12:11:39,863] Trial 93 finished with value: 0.8032297093539629 and parameters: {'n_estimators': 367, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 86 with value: 0.8037974804215503.\n[I 2025-09-12 12:11:40,899] Trial 94 finished with value: 0.803572739362336 and parameters: {'n_estimators': 341, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 86 with value: 0.8037974804215503.\n[I 2025-09-12 12:11:41,860] Trial 95 finished with value: 0.8043518893660696 and parameters: {'n_estimators': 316, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 95 with value: 0.8043518893660696.\n[I 2025-09-12 12:11:42,822] Trial 96 finished with value: 0.8036294131946595 and parameters: {'n_estimators': 314, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 95 with value: 0.8043518893660696.\n[I 2025-09-12 12:11:43,779] Trial 97 finished with value: 0.8041311045121395 and parameters: {'n_estimators': 315, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 95 with value: 0.8043518893660696.\n[I 2025-09-12 12:11:44,765] Trial 98 finished with value: 0.8039075709693974 and parameters: {'n_estimators': 325, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 95 with value: 0.8043518893660696.\n[I 2025-09-12 12:11:45,707] Trial 99 finished with value: 0.8032945339224742 and parameters: {'n_estimators': 310, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 95 with value: 0.8043518893660696.\n\n\n\n在Top 10特征集上的贝叶斯优化完成!\n找到的最佳超参数组合 (Best Parameters): \n{'n_estimators': 316, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'log2'}\n\n最终模型的最佳平均 AUC 分数是: 0.8044\n\n正在使用找到的最佳特征和最佳超参数来训练最终模型...\n最终模型训练完成！这个模型是我们整个分析过程的最终成果。\n\n\n同样运行了好几次，得到了较好的数据，是最终模型的最佳平均 AUC 分数是: 0.8044\nAUC 接近 0.805 通常意味着模型已经捕捉到了数据中大部分的可预测结构。这就是我们在机器学习中常说的“性能瓶颈”：再怎么调参、选特征，提升空间已经非常有限，这是我们随机森林的极限了，除非换一个更加适配这个数据的机器学习模型。\n模型的性能上限，往往不是由算法决定的，而是由数据本身的可预测性决定的。 如果数据中没有更多“可解释的信号”，再复杂的模型也无法凭空创造预测能力。\n\n1. 概念引入：性能天花板与不可约减误差 (Irreducible Error)🧉\n性能天花板（Performance Ceiling）。”\n核心知识点：\n在机器学习中，一个模型的总误差可以被分解为两部分：\n总误差 = 可约减误差 (Reducible Error) + 不可约减误差 (Irreducible Error)\n\n可约减误差： 这是我们可以通过选择更好的模型、做特征工程、调优超参数来减少的部分。我们之前做的所有工作，都是在努力将这部分误差降到最低。\n不可约减误差： 这是数据本身固有的、无法被任何模型消除的“噪声”或不确定性。这个误差决定了我们性能天花板的高度。\n打个比方 (Analogy):\n这就像用麦克风录音。我们可以买更好的麦克风、调整录音棚的环境来减少背景噪音（降低可约减误差）。但是，声音本身的一些微小颤动、空气中的随机分子运动是无法被消除的（不可约减误差）。我们能做的，就是尽可能清晰地捕捉到“信号”，但我们无法消除所有的“噪声”。\n\n\n\n教学总结：当我们触及天花板时，该做什么🤖？\n这是最关键的教学部分。当模型调优不再有效时，真正的数据科学家会从以下几个方面思考，这也是未来的工作方向：\n路径 A：获取更好的数据 (数据为中心 - The Data-Centric Approach)\n这是最重要、最有效的路径。\n\n行动： 与领域专家（精神科医生、心理学家）合作，讨论是否可以收集新的、可能与主观幸福感相关的数据。比如，在下一次研究中加入“社会支持量表”、“生活事件记录”等。\n结论： 提升性能天花板的最好方法，是为模型提供更高信噪比、更丰富信息的数据。\n\n路径 B：尝试完全不同的模型 (模型为中心 - The Model-Centric Approach)\n\n行动： 我们已经证明了随机森林（一种基于决策树的集成模型）的极限。我们可以尝试结构完全不同的模型，比如：\n\n深度学习/神经网络 (Deep Learning / Neural Networks): 如果有足够多的数据，神经网络能够捕捉到比树模型更高阶、更复杂的非线性关系。\n梯度提升机 (Gradient Boosting Machines): 我们可以对之前测试过的 XGBoost 或 LightGBM 模型进行同样深入的特征选择和贝叶斯优化，有时它们在特定数据集上的表现会略胜于随机森林。\n\n结论： 换一个更强大的模型有可能会略微提升性能，但这通常是在数据质量已经很好的前提下。如果数据本身信息量有限，换模型也只是“锦上添花”，而不会有质的飞跃。"
  },
  {
    "objectID": "index.html#论文总结",
    "href": "index.html#论文总结",
    "title": "SEM + ML 实际项目论文复现",
    "section": "论文总结🤗",
    "text": "论文总结🤗\nthis is what it is 没什么总结的，我们的机器学习得到了在我们测量的所有事物中，哪些与患者的主观幸福感最密切相关？即 OCS、躯体化、认知和抑郁是最重要的预测因素。我们确定了主要特征\n这也是差不多你们要写的论文的机器学习的大概流程\n\n但是如果看我们的复现论文，他们接下来的步骤是\n\n\nStep 1: Network Analysis (网络分析)🫥\n“为什么”（目的）：\n机器学习告诉我们，强迫症症状、躯体化症状、抑郁症和认知是最重要的预测因素。但它并没有告诉我们它们是如何相互关联的。强迫症症状和躯体化症状是否高度相关？抑郁症是否是连接一切的中心枢纽？\n网络分析帮助我们将症状的“生态系统”可视化。它将每个变量（症状、人口统计因素）视为一个“节点”（城市），并将它们之间的相关性视为一条“边”（高速公路）。相关性越强，高速公路就越粗。\n论文中的图3）：\n\n社群（群落）： 他们寻找的是节点之间的连接比与网络其他部分的连接更紧密的集群。在论文中，他们发现了一个关键模式：\n\n社区 1： 主观幸福感（SWB）、强迫症\n社区 2： 精神病症状（阳性、阴性）、认知缺陷和社会心理功能 (PSP) 形成了另一个不同的集群。\n幸福感的内在体验与强迫症和躯体系统更密切相关\n\n中心性： 该指标用于识别网络中最具“影响力”的节点。论文指出， 抑郁症的“中介中心性”最高。\n\n这是什么意思？ 这意味着抑郁症是连接两个群体的关键桥梁 。要从“精神病症状”群体走向“主观幸福感”群体，通常需要经过“抑郁症”这一环节。这表明抑郁症起着关键的中介作用。\n\n\n这是一种可视化和探索性的工具。它从我们的机器学习模型中获取重要特征列表，并将其转化为地图。这张地图向我们展示了复杂症状网络的结构和关键因素，这让我们假设抑郁症是一个关键的中介因素。\n\n\nStep 2: Structural Equation Modeling (SEM) (结构方程模型)🍔\n方向或因果关系。它显示了两座城市之间的高速公路，但没有显示交通走向\n看看论文中的图 4。这是他们的 SEM 分析结果。它就像一个流程图，展示了影响力的流动。\n\n直接效应（直接效应）： 从一个变量到另一个变量的直线箭头。\n\n论文发现， 强迫性行为对主观幸福感（SWB）的直接负面影响最为强烈 （见粗红色箭头，系数为-0.59）。这是论文最重要的发现：高强迫性行为会直接摧毁患者的幸福感。\n\n间接效应： 至少经过一个其他变量的路径。\n\n论文证实了抑郁症的“桥梁”作用。例如，阴性症状有一个箭头指向抑郁症，抑郁症也有一个箭头指向主观幸福感。这是一条间接路径 。这意味着阴性症状部分地通过增加抑郁来恶化主观幸福感 。\nOCS 还通过抑郁症对 SWB 产生间接影响 。\n\n\nSEM 是一种验证性工具，是分析的最终步骤，也是最有力的一步。它超越了“什么是重要的”（ML）和“它们如何关联”（网络分析）的范畴，从而检验了关于“什么导致什么”的特定理论。SEM 结果为本文的主要结论提供了最有力的证据：管理 OCS 对于改善精神分裂症患者的主观幸福感至关重要，这既是因为它强大的直接影响，也是因为它通过抑郁症产生的间接影响。"
  }
]