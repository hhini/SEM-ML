{
  "hash": "ac31b6f33c3035a1aa05b645a5816098",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 实际项目论文复现\nauthors:\n  - name: Norah Jones\n    affiliation: The University\n    roles: writing\n    corresponding: true\nbibliography: references.bib\n---\n\n# SEM加ML分析 🫥\n\n------------------------------------------------------------------------\n\n***要具体知道为什么要结合，可以参考文献 [混合人工神经网络与结构方程模型技术：综述](https://link.springer.com/article/10.1007/s40747-021-00503-w)Hybrid artificial neural network and structural equation modelling techniques: a survey***\n\n## 为什么要结合机器学习（ML）与结构方程模型（SEM）\n\n### ✨ 理论与实践的融合：SEM + ML 的优势\n\n结构方程模型（SEM）是一种强大的因果建模工具，能够同时处理测量模型（潜变量）和路径模型（变量间关系）。但在实际研究中，尤其是面对高维数据(就是变量多)、复杂变量结构或非线性关系时，SEM也面临挑战：\n\n-   **变量筛选困难**：传统SEM依赖理论假设，变量选择主观性强(一般要参考社会科学文献，并且进行假设，麻了)；\n\n-   **模型拟合受限**：当变量数量多、关系复杂时，SEM模型容易过拟合或拟合不佳(过拟合和欠拟合)；\n\n-   **预测能力有限**：SEM更偏向解释性而非预测性。\n\n而机器学习（ML）正好可以补足这些短板：\n\n| 功能       | SEM         | ML                |\n|------------|-------------|-------------------|\n| 因果推理   | ✅ 强       | ❌ 弱（但可辅助） |\n| 潜变量建模 | ✅ 支持     | ❌ 不直接支持     |\n| 特征筛选   | ❌ 依赖理论 | ✅ 数据驱动       |\n| 非线性建模 | ❌ 有限     | ✅ 强大           |\n| 预测能力   | ❌ 弱       | ✅ 强             |\n\n因此，将ML与SEM结合，可以实现：\n\n-   **数据驱动的变量筛选**（如用随机森林或SHAP值找出最重要的预测因子(一般来说如果变量较多的时候，一般是不降维的，而是筛选变量的）；\n\n-   **提升模型解释力与预测力**（先用ML找出**关键变量**，再用SEM建模路径）；\n\n-   **增强模型稳健性**（ML可用于**验证**SEM模型的泛化能力）；\n\n-   **提高研究影响力**（结合方法更前沿，易发表在高影响因子期刊）。\n\n### 📈 文献支持：结合ML与SEM的研究越来越多，影响因子也在上升\n\n以下是提供的几篇代表性文献，展示了ML+SEM的实际应用与发表潜力：\n\n1.  ✅ **复现的论文**： PMC12236327\n\n    -   结合随机森林与SEM，识别影响精神分裂症患者主观幸福感的关键因素。\n\n    -   通过ML筛选变量，再用lavaan构建路径模型，实现因果解释与预测并重。\n\n    -   发表在《Translational Psychiatry》，IF ≈ 7.7。\n\n2.  📱 **智能穿戴设备采纳研究**： Bou Nassif et al. (2022)\n\n    -   使用PLS-SEM与ML对智能手表采纳意图进行建模。\n\n    -   ML揭示“用户满意度”是最关键预测因子，SEM验证其路径关系。\n\n    -   发表在《Heliyon》，IF ≈ 4.0。\n\n3.  🧠 **消费者心理与行为研究**： ScienceDirect 2023论文\n\n    -   用ML筛选影响消费者信任的变量，再用SEM建模信任形成机制。\n\n    -   发表在《Journal of Business Research》，IF ≈ 10.0。\n\n4.  🧬 **健康行为预测研究**： ScienceDirect 2024论文\n\n    -   结合ML与SEM预测健康行为采纳，强调数据驱动与理论建模的融合。\n\n    -   发表在《Preventive Medicine Reports》，IF ≈ 3.5。\n\n------------------------------------------------------------------------\n\n## 什么是机器学习🫤\n\n在正式进入机器学习（Machine Learning, ML）＋结构方程模型（SEM）的整合应用之前，首先要让大家对“机器学习”这个概念有一个直观的理解。不必过分纠结复杂的数学公式，而要抓住它的核心思想和研究流程。\n\n### 2.1 机器学习的定义🤤\n\n机器学习是一种***让计算机通过“样本数据”自行发现规律***，***并将这些规律用于“预测”或“决策”的技术。***\\\n- 它不像传统统计学那样强调整体假设检验和参数估计，更注重“从数据中自动学习”。\\\n- ML 旨在用已有数据训练一个模型，让模型在新数据上也能表现出良好的预测或分类能力。\n\n***这也让机器学习无法用数学公式和因果关系来知道电脑是怎么想的，形成了所谓的黑匣子，解释性太差，但是用来预测就没毛病***\n\n### 2.2 ML 与传统统计方法的差异🌮\n\n-   目标不同\n    -   统计学：偏重解释性，解析变量间的因果或关联；\\\n    -   机器学习：偏重预测性，追求模型的泛化能力，即给我数据我可以预测出大概的，符合之前数据的答案。\n-   模型假设\n    -   统计模型：通常要求满足正态性、线性关系等严格假设；\\\n    -   ML 模型：对数据分布和关系的假设更弱，***可以处理非线性、高维度数据***。\n-   关注点\n    -   统计方法：估计参数、显著性检验、置信区间；\\\n    -   ML 方法：***模型选择、交叉验证、过拟合控制、预测误差***。\n\n### 2.3 机器学习的主要类型🤖\n\n1.  **监督学习（Supervised Learning）**\n    -   有“标签”的数据（例如：患者有/无压力性损伤）。\\\n    -   常见算法：逻辑回归、决策树、随机森林、支持向量机、神经网络等。\\\n    -   核心任务：分类（Classification）和回归（Regression）。\n2.  **无监督学习（Unsupervised Learning）**\n    -   无标签数据。\\\n    -   常见算法：聚类（K-means、层次聚类）、降维（PCA、t-SNE）。\\\n    -   核心任务：发现数据的内在结构、分群、特征降维。\n3.  **强化学习（Reinforcement Learning）**\n    -   通过“试错”与环境互动来学习最优策略。\\\n    -   在护理研究中应用相对少，可用于优化护理路径或资源调度。\n\n### 2.4 机器学习研究流程🛫\n\n1.  **数据准备与清洗**\n    -   汇总临床指标、问卷量表、电子健康记录等；\\\n    -   处理缺失值、异常值、分类变量编码。\n2.  **特征工程**\n    -   选择或构造与研究目标最相关的变量（特征）即挑选重要的特征变量，过滤不需要的变量；\\\n    -   归一化、标准化、one-hot 编码(独热编码)等。\n3.  **模型训练与优化**\n    -   在“训练集”上训练模型；\\\n    -   通过交叉验证（Cross-Validation）选择超参数，**防止过拟合**(过拟合即是对模型来说，学习太多不重要的特征，比如我训练了一个识别学长还是学姐的模型，但是模型学习到了几个女装的学长，本来女装这是几个人的特殊癖好，但是模型认为女装的都是学长，这扯不扯）。\n4.  **模型评估**\n    -   ***在“测试集”或“留出数据”上评估预测性能***，这也是为什么机器学习都要分成训练集和验证集，要么7/3,要么8/2等等（千万别耍小聪明，觉得数据不够多，还要拆分，耍滑头把所有数据都拿去训练，然后再拿其中一部分来预测，因为模型有记忆的，你预测训练过的数据，肯定准确率是百分百，就好像你明明知道明天的天气，那为什么要根据之前的数据推测呢）（如 AUC、准确率、RMSE）；\\\n    -   比较不同算法的优劣。\n5.  **模型解释与应用**\n    -   使用 SHAP、LIME 等方法**解释模型**\n\n        它们是两种常用的“模型解释工具”，可以告诉我们：**哪些变量对模型预测最重要，以及它们的影响方向**\n\n------------------------------------------------------------------------\n\n## 复现一篇论文🤖\n\n**`最好学习的方法就是看相关论文，然后学习论文知识，然后复现学习，所以啥也不说了，直接真刀真枪开干就完了`**\n\n#### Replicating Research on Subjective Well-Being in Schizophrenia: A Guide to Generating Synthetic Data for SEM and Machine Learning 精神分裂症患者主观幸福感研究：SEM 和机器学习合成数据生成指南[论文链接](https://pmc.ncbi.nlm.nih.gov/articles/PMC12236327/#s5)\n\n***！现在我不以社会科学的角度解析这篇文章，只用统计学和代码来解析如何处理***\n\n因为我现在没有相关的数据，所以我只能用代码模拟和文章差不多的数据\n\n-   **表 1：机器学习模型中包含的独立变量列表：** 该表概述了分析中使用的所有变量，包括社会人口因素（年龄、性别、教育、职业）、疾病相关因素（疾病持续时间）和各种临床测量。\n\n-   **表 2：样本特征：**提供了 637 名患者原始数据集中变量的描述性统计数据。它包括连续变量的平均值、标准差和范围，以及分类变量的频率。\n\n-   **图 3：网络模型和中心性指标：**展示了不同变量之间的相关性。连接及其粗细表示关系的强度，这应该反映在合成数据的相关矩阵中。\n\n-   **图 4：结构方程模型（SEM）分析：** 该图比网络分析更进一步，揭示了变量之间的因果路径和效应方向。路径系数量化了这些关系的强度和方向。\n\n#### 使用 Python 生成合成数据\n\n以下 Python 代码演示了如何生成一个与论文中描述的特征近似的合成数据集。此代码使用 numpy 和 pandas 库创建一个数据框，其中包含与原始研究中的变量具有相似统计特性和相关性的变量。变量不用Chinese\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# 导入所需的库\n# 导入所需的库\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import truncnorm\n\n# --- 1. 定义样本量 ---\nn_patients = 637\n\n# --- 2. 基于SEM和网络分析生成核心相关数据 ---\ncorrelation_matrix = np.array([\n    # SWB,   OCS,   Som,   Cog,   Dep,   Neg,   Pos,   PSP\n    [1.00, -0.59, -0.40, -0.20, -0.35, -0.25, -0.15,  0.30],\n    [-0.59,  1.00,  0.70,  0.30,  0.40,  0.20,  0.18, -0.25],\n    [-0.40,  0.70,  1.00,  0.25,  0.35,  0.15,  0.10, -0.20],\n    [-0.20,  0.30,  0.25,  1.00,  0.26,  0.48,  0.36, -0.40],\n    [-0.35,  0.40,  0.35,  0.26,  1.00,  0.35,  0.24, -0.50],\n    [-0.25,  0.20,  0.15,  0.48,  0.35,  1.00,  0.22, -0.60],\n    [-0.15,  0.18,  0.10,  0.36,  0.24,  0.22,  1.00, -0.30],\n    [ 0.30, -0.25, -0.20, -0.40, -0.50, -0.60, -0.30,  1.00]\n])\n\nmean = np.zeros(correlation_matrix.shape[0])\nnp.random.seed(42)\ncorrelated_data = np.random.multivariate_normal(mean, correlation_matrix, size=n_patients)\n\n# --- 3. 缩放和转换数据以匹配论文表2中的统计特征 ---\n\ndef scale_variable(data_column, mean, std, min_val, max_val):\n    scaled = data_column * std + mean\n    return np.clip(scaled, min_val, max_val)\n\ncgi_pos = scale_variable(correlated_data[:, 6], 3.7, 1.4, 1, 7)\ncgi_neg = scale_variable(correlated_data[:, 5], 3.3, 1.2, 1, 7)\ncgi_dep = scale_variable(correlated_data[:, 4], 2.6, 1.2, 1, 6)\ncgi_cog = scale_variable(correlated_data[:, 3], 3.1, 1.1, 1, 6)\nscl_som = scale_variable(correlated_data[:, 2], 21.3, 8.7, 12, 58)\nscl_ocs = scale_variable(correlated_data[:, 1], 22.4, 8.3, 10, 50)\nswn_total = scale_variable(correlated_data[:, 0], 73.5, 17.6, 24, 120)\npsp_total = scale_variable(correlated_data[:, 7], 55.9, 14.8, 10, 90)\ndiepss_severity = np.random.uniform(0, 4, n_patients)\ndiepss_severity = np.clip(diepss_severity, 0, 4)\ndiepss_severity_mean = np.mean(diepss_severity)\ndiepss_severity = (diepss_severity - diepss_severity_mean) * (0.8 / np.std(diepss_severity)) + 0.6\ndiepss_severity = np.clip(diepss_severity, 0, 4)\n\n# --- 4. 生成社会人口统计学及其他变量 ---\n\nage_mean, age_std, age_min, age_max = 35.7, 10.5, 18, 64\nage = truncnorm.rvs((age_min - age_mean) / age_std, (age_max - age_mean) / age_std, loc=age_mean, scale=age_std, size=n_patients)\nsex = np.random.choice([1, 0], size=n_patients, p=[0.529, 0.471])\neducation = np.random.choice(['Less than 12 years', 'High school', 'College or graduate'], size=n_patients, p=[0.171, 0.413, 0.416])\noccupation = np.random.choice([1, 0], size=n_patients, p=[0.174, 0.826])\nillness_duration_mean, illness_duration_std, illness_duration_min, illness_duration_max = 9.1, 8.1, 0, 44\nillness_duration = truncnorm.rvs((illness_duration_min - illness_duration_mean) / illness_duration_std, (illness_duration_max - illness_duration_mean) / illness_duration_std, loc=illness_duration_mean, scale=illness_duration_std, size=n_patients)\nobesity = np.random.choice([1, 0], size=n_patients, p=[0.468, 0.532])\nhypertension = np.random.choice([1, 0], size=n_patients, p=[0.273, 0.727])\ndiabetes = np.random.choice([1, 0], size=n_patients, p=[0.154, 0.846])\n\n# --- 5. 组合成 Pandas DataFrame ---\nsynthetic_df = pd.DataFrame({\n    'Age': age.round(1),\n    'Sex_Female': sex,\n    'Education': education,\n    'Occupation_Employed': occupation,\n    'Duration_of_Illness': illness_duration.round(1),\n    'Obesity': obesity,\n    'Hypertension': hypertension,\n    'Diabetes': diabetes,\n    'CGI_Positive': cgi_pos.round(1),\n    'CGI_Negative': cgi_neg.round(1),\n    'CGI_Depressive': cgi_dep.round(1),\n    'CGI_Cognitive': cgi_cog.round(1),\n    'SCL90R_Somatization': scl_som.round(1),\n    'SCL90R_OCS': scl_ocs.round(1),\n    'SWN_Total_Score': swn_total.round(1),\n    'DIEPSS_Overall_Severity': diepss_severity.round(1),\n    'PSP_Total_Score': psp_total.round(1)\n})\n\n# --- 6. 将数据框 (DataFrame) 保存为 CSV 文件 ---\n# !!! 重要提示: 请将下面的路径修改为您希望保存文件的实际位置 !!!\n# 例如 Windows: \"C:/Users/用户名/Documents/synthetic_data.csv\"\n# 例如 macOS: \"/Users/用户名/Documents/synthetic_data.csv\"\nfile_path = \"./synthetic_schizophrenia_data.csv\"\n\n# 使用 to_csv() 函数来保存数据\n# 参数 index=False 的意思是，数据框的行索引 (0, 1, 2, ...) 不会被写入到 CSV 文件中\nsynthetic_df.to_csv(file_path, index=False)\n\nprint(synthetic_df.head(10))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Age  Sex_Female            Education  Occupation_Employed  \\\n0  21.5           1          High school                    0   \n1  29.3           0          High school                    0   \n2  48.7           0  College or graduate                    0   \n3  18.9           1          High school                    0   \n4  28.2           0          High school                    1   \n5  53.8           1   Less than 12 years                    1   \n6  39.2           0          High school                    0   \n7  30.0           1          High school                    1   \n8  22.8           0          High school                    1   \n9  20.4           1          High school                    0   \n\n   Duration_of_Illness  Obesity  Hypertension  Diabetes  CGI_Positive  \\\n0                 13.7        0             0         0           2.7   \n1                 16.2        0             0         0           3.3   \n2                 11.0        1             0         1           4.7   \n3                 12.1        1             0         0           4.0   \n4                  5.0        1             1         0           3.5   \n5                  6.6        1             0         0           3.9   \n6                  4.2        0             1         0           4.8   \n7                  5.9        1             0         0           2.3   \n8                  5.4        0             0         0           3.0   \n9                 10.8        1             0         0           5.1   \n\n   CGI_Negative  CGI_Depressive  CGI_Cognitive  SCL90R_Somatization  \\\n0           5.1             2.8            4.0                 23.6   \n1           1.1             2.7            3.5                 17.5   \n2           1.7             1.8            1.8                 14.4   \n3           2.1             1.8            3.5                 17.2   \n4           2.8             4.0            3.5                 12.0   \n5           3.1             4.1            3.9                 26.0   \n6           5.1             3.4            3.3                 14.7   \n7           2.6             1.6            3.4                 20.2   \n8           3.5             2.8            3.8                 28.0   \n9           1.6             1.4            4.0                 36.0   \n\n   SCL90R_OCS  SWN_Total_Score  DIEPSS_Overall_Severity  PSP_Total_Score  \n0        27.8             75.3                      0.0             59.8  \n1        19.3             65.1                      1.1             61.6  \n2        12.9             61.6                      1.9             71.8  \n3        26.7             92.3                      0.0             64.4  \n4        15.6             69.6                      1.5             41.9  \n5        30.6             80.2                      0.0             48.2  \n6        19.3             92.1                      0.7             42.7  \n7        12.2             87.2                      0.0             57.5  \n8        39.0             56.1                      1.8             61.8  \n9        25.8             62.3                      1.4             81.9  \n```\n:::\n:::\n\n\n:::{#58c12541 .cell .markdown}\n### 🧮 数据维度\n\n-   **样本数（行数）**：`637` 由 `n_patients = 637` 指定，模拟的是637位精神分裂症患者的临床与人口学数据。\n\n-   **变量数（列数）**：`17` 包括临床症状、人口学特征、功能状态和代谢指标等变量。\n\n### 📊 变量列表与类型\n\n| 变量名                    | 类型     | 描述                        |\n|---------------------------|----------|-----------------------------|\n| `Age`                     | 连续变量 | 年龄（18–64岁）             |\n| `Sex_Female`              | 二元变量 | 性别（1=女性，0=男性）      |\n| `Education`               | 分类变量 | 教育水平（3类）             |\n| `Occupation_Employed`     | 二元变量 | 是否就业（1=在职）          |\n| `Duration_of_Illness`     | 连续变量 | 病程（0–44年）              |\n| `Obesity`                 | 二元变量 | 是否肥胖                    |\n| `Hypertension`            | 二元变量 | 是否高血压                  |\n| `Diabetes`                | 二元变量 | 是否糖尿病                  |\n| `CGI_Positive`            | 连续变量 | 阳性症状评分（1–7）         |\n| `CGI_Negative`            | 连续变量 | 阴性症状评分（1–7）         |\n| `CGI_Depressive`          | 连续变量 | 抑郁症状评分（1–6）         |\n| `CGI_Cognitive`           | 连续变量 | 认知缺陷评分（1–6）         |\n| `SCL90R_Somatization`     | 连续变量 | 躯体化评分（12–58）         |\n| `SCL90R_OCS`              | 连续变量 | 强迫症状评分（10–50）       |\n| `SWN_Total_Score`         | 连续变量 | 主观幸福感评分（24–120）    |\n| `DIEPSS_Overall_Severity` | 连续变量 | 锥体外系不良反应评分（0–4） |\n| `PSP_Total_Score`         | 连续变量 | 社会功能评分（10–90）       |\n:::\n\n::: {.cell execution_count=2}\n``` {.python .cell-code .hidden}\na = 100\nprint(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n100\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code .hidden}\nprint(\"nihao\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnihao\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nprint(\"Hello world\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHello world\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Generate synthetic data\nnp.random.seed(42)\nn = 100\ndata = pd.DataFrame({\n    'Feature A': np.random.normal(loc=0, scale=1, size=n),\n    'Feature B': np.random.normal(loc=5, scale=2, size=n),\n    'Feature C': np.random.normal(loc=-3, scale=1.5, size=n),\n    'Class': np.random.choice(['Class 1', 'Class 2', 'Class 3'], size=n)\n})\n\n# Visualize with seaborn pairplot\nsns.set(style=\"whitegrid\")\nplot = sns.pairplot(data, hue='Class', palette='Set2', diag_kind='kde')\nplt.suptitle(\"Synthetic Data Visualization\", y=1.02)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-pdf/cell-6-output-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": []
  }
}