{
  "hash": "111af1ec32e2a0d5ab7fecf1e77aa9b3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: SEM + ML 实际项目论文复现\nauthors:\n  - name: JAYZJAYZ\n    affiliation: GAN NAN medical university\n    roles: writing\n    corresponding: true\nbibliography: references.bib\n---\n\n# SEM加ML分析 🫥\n\n------------------------------------------------------------------------\n\n***要具体知道为什么要结合，可以参考文献 [混合人工神经网络与结构方程模型技术：综述](https://link.springer.com/article/10.1007/s40747-021-00503-w)Hybrid artificial neural network and structural equation modelling techniques: a survey***\n\n## 为什么要结合机器学习（ML）与结构方程模型（SEM）\n\n### ✨ 理论与实践的融合：SEM + ML 的优势\n\n结构方程模型（SEM）是一种强大的因果建模工具，能够同时处理测量模型（潜变量）和路径模型（变量间关系）。但在实际研究中，尤其是面对高维数据(就是变量多)、复杂变量结构或非线性关系时，SEM也面临挑战：\n\n-   **变量筛选困难**：传统SEM依赖理论假设，变量选择主观性强(一般要参考社会科学文献，并且进行假设，麻了)；\n\n-   **模型拟合受限**：当变量数量多、关系复杂时，SEM模型容易过拟合或拟合不佳(过拟合和欠拟合)；\n\n-   **预测能力有限**：SEM更偏向解释性而非预测性。\n\n而机器学习（ML）正好可以补足这些短板：\n\n| 功能       | SEM         | ML                |\n|------------|-------------|-------------------|\n| 因果推理   | ✅ 强       | ❌ 弱（但可辅助） |\n| 潜变量建模 | ✅ 支持     | ❌ 不直接支持     |\n| 特征筛选   | ❌ 依赖理论 | ✅ 数据驱动       |\n| 非线性建模 | ❌ 有限     | ✅ 强大           |\n| 预测能力   | ❌ 弱       | ✅ 强             |\n\n因此，将ML与SEM结合，可以实现：\n\n-   **数据驱动的变量筛选**（如用随机森林或SHAP值找出最重要的预测因子(一般来说如果变量较多的时候，一般是不降维的，而是筛选变量的）；\n\n-   **提升模型解释力与预测力**（先用ML找出**关键变量**，再用SEM建模路径）；\n\n-   **增强模型稳健性**（ML可用于**验证**SEM模型的泛化能力）；\n\n-   **提高研究影响力**（结合方法更前沿，易发表在高影响因子期刊）。\n\n### 📈 文献支持：结合ML与SEM的研究越来越多，影响因子也在上升\n\n以下是提供的几篇代表性文献，展示了ML+SEM的实际应用与发表潜力：\n\n1.  ✅ **复现的论文**： PMC12236327\n\n    -   结合随机森林与SEM，识别影响精神分裂症患者主观幸福感的关键因素。\n\n    -   通过ML筛选变量，再用lavaan构建路径模型，实现因果解释与预测并重。\n\n    -   发表在《Translational Psychiatry》，IF ≈ 7.9。\n\n2.  📱 **智能穿戴设备采纳研究**： Bou Nassif et al. (2022)\n\n    -   使用PLS-SEM与ML对智能手表采纳意图进行建模。\n\n    -   ML揭示“用户满意度”是最关键预测因子，SEM验证其路径关系。\n\n    -   发表在《Heliyon》，IF ≈ 4.0。\n\n3.  🧠 **消费者心理与行为研究**： ScienceDirect 2023论文\n\n    -   用ML筛选影响消费者信任的变量，再用SEM建模信任形成机制。\n\n    -   发表在《Journal of Business Research》，IF ≈ 10.0。\n\n4.  🧬 **健康行为预测研究**： ScienceDirect 2024论文\n\n    -   结合ML与SEM预测健康行为采纳，强调数据驱动与理论建模的融合。\n\n    -   发表在《Preventive Medicine Reports》，IF ≈ 3.5。\n\n------------------------------------------------------------------------\n\n## 什么是机器学习🫤\n\n在正式进入机器学习（Machine Learning, ML）＋结构方程模型（SEM）的整合应用之前，首先要让大家对“机器学习”这个概念有一个直观的理解。不必过分纠结复杂的数学公式，而要抓住它的核心思想和研究流程。\n\n### 2.1 机器学习的定义🤤\n\n机器学习是一种***让计算机通过“样本数据”自行发现规律***，***并将这些规律用于“预测”或“决策”的技术。***\\\n- 它不像传统统计学那样强调整体假设检验和参数估计，更注重“从数据中自动学习”。\\\n- ML 旨在用已有数据训练一个模型，让模型在新数据上也能表现出良好的预测或分类能力。\n\n***这也让机器学习无法用数学公式和因果关系来知道电脑是怎么想的，形成了所谓的黑匣子，解释性太差，但是用来预测就没毛病***\n\n### 2.2 ML 与传统统计方法的差异🌮\n\n-   目标不同\n    -   统计学：偏重解释性，解析变量间的因果或关联；\\\n    -   机器学习：偏重预测性，追求模型的泛化能力，即给我数据我可以预测出大概的，符合之前数据的答案。\n-   模型假设\n    -   统计模型：通常要求满足正态性、线性关系等严格假设；\\\n    -   ML 模型：对数据分布和关系的假设更弱，***可以处理非线性、高维度数据***。\n-   关注点\n    -   统计方法：估计参数、显著性检验、置信区间；\\\n    -   ML 方法：***模型选择、交叉验证、过拟合控制、预测误差***。\n\n### 2.3 机器学习的主要类型🤖\n\n1.  **监督学习（Supervised Learning）**\n    -   有“标签”的数据（例如：患者有/无压力性损伤）。\\\n    -   常见算法：逻辑回归、决策树、随机森林、支持向量机、神经网络等。\\\n    -   核心任务：分类（Classification）和回归（Regression）。\n2.  **无监督学习（Unsupervised Learning）**\n    -   无标签数据。\\\n    -   常见算法：聚类（K-means、层次聚类）、降维（PCA、t-SNE）。\\\n    -   核心任务：发现数据的内在结构、分群、特征降维。\n3.  **强化学习（Reinforcement Learning）**\n    -   通过“试错”与环境互动来学习最优策略。\\\n    -   在护理研究中应用相对少，可用于优化护理路径或资源调度。\n\n### 2.4 机器学习研究流程🛫\n\n1.  **数据准备与清洗**\n    -   汇总临床指标、问卷量表、电子健康记录等；\\\n    -   处理缺失值、异常值、分类变量编码。\n2.  **特征工程**\n    -   选择或构造与研究目标最相关的变量（特征）即挑选重要的特征变量，过滤不需要的变量；\\\n    -   归一化、标准化、one-hot 编码(独热编码)等。\n3.  **模型训练与优化**\n    -   在“训练集”上训练模型；\\\n    -   通过交叉验证（Cross-Validation）选择超参数，**防止过拟合**(过拟合即是对模型来说，学习太多不重要的特征，比如我训练了一个识别学长还是学姐的模型，但是模型学习到了几个女装的学长，本来女装这是几个人的特殊癖好，但是模型认为女装的都是学长，这扯不扯）。\n4.  **模型评估**\n    -   ***在“测试集”或“留出数据”上评估预测性能***，这也是为什么机器学习都要分成训练集和验证集，要么7/3,要么8/2等等（千万别耍小聪明，觉得数据不够多，还要拆分，耍滑头把所有数据都拿去训练，然后再拿其中一部分来预测，因为模型有记忆的，你预测训练过的数据，肯定准确率是百分百，就好像你明明知道明天的天气，那为什么要根据之前的数据推测呢）（如 AUC、准确率、RMSE）；\\\n    -   比较不同算法的优劣。\n5.  **模型解释与应用**\n    -   使用 SHAP、LIME 等方法**解释模型**\n\n        它们是两种常用的“模型解释工具”，可以告诉我们：**哪些变量对模型预测最重要，以及它们的影响方向**\n\n------------------------------------------------------------------------\n\n# 复现一篇论文🤖\n\n**`最好学习的方法就是看相关论文，然后学习论文知识，然后复现学习，所以啥也不说了，直接真刀真枪开干就完了`**\n\n#### Replicating Research on Subjective Well-Being in Schizophrenia: A Guide to Generating Synthetic Data for SEM and Machine Learning 精神分裂症患者主观幸福感研究：SEM 和机器学习合成数据生成指南\n\n#### [论文链接](https://pmc.ncbi.nlm.nih.gov/articles/PMC12236327/#s5) 2024 Sep 7 ; 51(4):1118–1133. doi: 10.1093/schbul/sbae156 - IF=7.9(我也不知道为什么这种文章有7分，做生信累死累活还拿不到5分，吐槽一下，麻了🥶）\n\n***！现在我不以社会科学的角度解析这篇文章，只用统计学和代码来解析如何处理***\n\n因为我现在没有相关的数据，所以我只能用代码模拟和文章差不多的数据\n\n-   **表 1：机器学习模型中包含的独立变量列表：** 该表概述了分析中使用的所有变量，包括社会人口因素（年龄、性别、教育、职业）、疾病相关因素（疾病持续时间）和各种临床测量。\n\n-   **表 2：样本特征：**提供了 637 名患者原始数据集中变量的描述性统计数据。它包括连续变量的平均值、标准差和范围，以及分类变量的频率。\n\n-   **图 3：网络模型和中心性指标：**展示了不同变量之间的相关性。连接及其粗细表示关系的强度，这应该反映在合成数据的相关矩阵中。\n\n-   **图 4：结构方程模型（SEM）分析：** 该图比网络分析更进一步，揭示了变量之间的因果路径和效应方向。路径系数量化了这些关系的强度和方向。\n\n#### 使用 Python 生成合成数据\n\n以下 Python 代码演示了如何生成一个与论文中描述的特征近似的合成数据集。此代码使用 numpy 和 pandas 库创建一个数据框，其中包含与原始研究中的变量具有相似统计特性和相关性的变量。变量不用Chinese\n\n::: {#59340f4d .cell execution_count=1}\n``` {.python .cell-code}\n# 导入所需的库\n# 导入所需的库\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import truncnorm\n\n# --- 1. 定义样本量 ---\nn_patients = 637\n\n# --- 2. 基于SEM和网络分析生成核心相关数据 ---\ncorrelation_matrix = np.array([\n    # SWB,   OCS,   Som,   Cog,   Dep,   Neg,   Pos,   PSP\n    [1.00, -0.59, -0.40, -0.20, -0.35, -0.25, -0.15,  0.30],\n    [-0.59,  1.00,  0.70,  0.30,  0.40,  0.20,  0.18, -0.25],\n    [-0.40,  0.70,  1.00,  0.25,  0.35,  0.15,  0.10, -0.20],\n    [-0.20,  0.30,  0.25,  1.00,  0.26,  0.48,  0.36, -0.40],\n    [-0.35,  0.40,  0.35,  0.26,  1.00,  0.35,  0.24, -0.50],\n    [-0.25,  0.20,  0.15,  0.48,  0.35,  1.00,  0.22, -0.60],\n    [-0.15,  0.18,  0.10,  0.36,  0.24,  0.22,  1.00, -0.30],\n    [ 0.30, -0.25, -0.20, -0.40, -0.50, -0.60, -0.30,  1.00]\n])\n\nmean = np.zeros(correlation_matrix.shape[0])\nnp.random.seed(42)\ncorrelated_data = np.random.multivariate_normal(mean, correlation_matrix, size=n_patients)\n\n# --- 3. 缩放和转换数据以匹配论文表2中的统计特征 ---\n\ndef scale_variable(data_column, mean, std, min_val, max_val):\n    scaled = data_column * std + mean\n    return np.clip(scaled, min_val, max_val)\n\ncgi_pos = scale_variable(correlated_data[:, 6], 3.7, 1.4, 1, 7)\ncgi_neg = scale_variable(correlated_data[:, 5], 3.3, 1.2, 1, 7)\ncgi_dep = scale_variable(correlated_data[:, 4], 2.6, 1.2, 1, 6)\ncgi_cog = scale_variable(correlated_data[:, 3], 3.1, 1.1, 1, 6)\nscl_som = scale_variable(correlated_data[:, 2], 21.3, 8.7, 12, 58)\nscl_ocs = scale_variable(correlated_data[:, 1], 22.4, 8.3, 10, 50)\nswn_total = scale_variable(correlated_data[:, 0], 73.5, 17.6, 24, 120)\npsp_total = scale_variable(correlated_data[:, 7], 55.9, 14.8, 10, 90)\ndiepss_severity = np.random.uniform(0, 4, n_patients)\ndiepss_severity = np.clip(diepss_severity, 0, 4)\ndiepss_severity_mean = np.mean(diepss_severity)\ndiepss_severity = (diepss_severity - diepss_severity_mean) * (0.8 / np.std(diepss_severity)) + 0.6\ndiepss_severity = np.clip(diepss_severity, 0, 4)\n\n# --- 4. 生成社会人口统计学及其他变量 ---\n\nage_mean, age_std, age_min, age_max = 35.7, 10.5, 18, 64\nage = truncnorm.rvs((age_min - age_mean) / age_std, (age_max - age_mean) / age_std, loc=age_mean, scale=age_std, size=n_patients)\nsex = np.random.choice([1, 0], size=n_patients, p=[0.529, 0.471])\neducation = np.random.choice(['Less than 12 years', 'High school', 'College or graduate'], size=n_patients, p=[0.171, 0.413, 0.416])\noccupation = np.random.choice([1, 0], size=n_patients, p=[0.174, 0.826])\nillness_duration_mean, illness_duration_std, illness_duration_min, illness_duration_max = 9.1, 8.1, 0, 44\nillness_duration = truncnorm.rvs((illness_duration_min - illness_duration_mean) / illness_duration_std, (illness_duration_max - illness_duration_mean) / illness_duration_std, loc=illness_duration_mean, scale=illness_duration_std, size=n_patients)\nobesity = np.random.choice([1, 0], size=n_patients, p=[0.468, 0.532])\nhypertension = np.random.choice([1, 0], size=n_patients, p=[0.273, 0.727])\ndiabetes = np.random.choice([1, 0], size=n_patients, p=[0.154, 0.846])\n\n# --- 5. 组合成 Pandas DataFrame ---\nsynthetic_df = pd.DataFrame({\n    'Age': age.round(1),\n    'Sex_Female': sex,\n    'Education': education,\n    'Occupation_Employed': occupation,\n    'Duration_of_Illness': illness_duration.round(1),\n    'Obesity': obesity,\n    'Hypertension': hypertension,\n    'Diabetes': diabetes,\n    'CGI_Positive': cgi_pos.round(1),\n    'CGI_Negative': cgi_neg.round(1),\n    'CGI_Depressive': cgi_dep.round(1),\n    'CGI_Cognitive': cgi_cog.round(1),\n    'SCL90R_Somatization': scl_som.round(1),\n    'SCL90R_OCS': scl_ocs.round(1),\n    'SWN_Total_Score': swn_total.round(1),\n    'DIEPSS_Overall_Severity': diepss_severity.round(1),\n    'PSP_Total_Score': psp_total.round(1)\n})\n\n# --- 6. 将数据框 (DataFrame) 保存为 CSV 文件 ---\n# !!! 重要提示: 请将下面的路径修改为您希望保存文件的实际位置 !!!\n# 例如 Windows: \"C:/Users/用户名/Documents/synthetic_data.csv\"\n# 例如 macOS: \"/Users/用户名/Documents/synthetic_data.csv\"\nfile_path = \"./synthetic_schizophrenia_data.csv\"\n\n# 使用 to_csv() 函数来保存数据\n# 参数 index=False 的意思是，数据框的行索引 (0, 1, 2, ...) 不会被写入到 CSV 文件中\nsynthetic_df.to_csv(file_path, index=False)\n\nprint(synthetic_df.head(10))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    Age  Sex_Female            Education  Occupation_Employed  \\\n0  21.5           1          High school                    0   \n1  29.3           0          High school                    0   \n2  48.7           0  College or graduate                    0   \n3  18.9           1          High school                    0   \n4  28.2           0          High school                    1   \n5  53.8           1   Less than 12 years                    1   \n6  39.2           0          High school                    0   \n7  30.0           1          High school                    1   \n8  22.8           0          High school                    1   \n9  20.4           1          High school                    0   \n\n   Duration_of_Illness  Obesity  Hypertension  Diabetes  CGI_Positive  \\\n0                 13.7        0             0         0           2.7   \n1                 16.2        0             0         0           3.3   \n2                 11.0        1             0         1           4.7   \n3                 12.1        1             0         0           4.0   \n4                  5.0        1             1         0           3.5   \n5                  6.6        1             0         0           3.9   \n6                  4.2        0             1         0           4.8   \n7                  5.9        1             0         0           2.3   \n8                  5.4        0             0         0           3.0   \n9                 10.8        1             0         0           5.1   \n\n   CGI_Negative  CGI_Depressive  CGI_Cognitive  SCL90R_Somatization  \\\n0           5.1             2.8            4.0                 23.6   \n1           1.1             2.7            3.5                 17.5   \n2           1.7             1.8            1.8                 14.4   \n3           2.1             1.8            3.5                 17.2   \n4           2.8             4.0            3.5                 12.0   \n5           3.1             4.1            3.9                 26.0   \n6           5.1             3.4            3.3                 14.7   \n7           2.6             1.6            3.4                 20.2   \n8           3.5             2.8            3.8                 28.0   \n9           1.6             1.4            4.0                 36.0   \n\n   SCL90R_OCS  SWN_Total_Score  DIEPSS_Overall_Severity  PSP_Total_Score  \n0        27.8             75.3                      0.0             59.8  \n1        19.3             65.1                      1.1             61.6  \n2        12.9             61.6                      1.9             71.8  \n3        26.7             92.3                      0.0             64.4  \n4        15.6             69.6                      1.5             41.9  \n5        30.6             80.2                      0.0             48.2  \n6        19.3             92.1                      0.7             42.7  \n7        12.2             87.2                      0.0             57.5  \n8        39.0             56.1                      1.8             61.8  \n9        25.8             62.3                      1.4             81.9  \n```\n:::\n:::\n\n\n### 🧮 数据维度\n\n-   **样本数（行数）**：`637` 由 `n_patients = 637` 指定，模拟的是637位精神分裂症患者的临床与人口学数据。\n\n-   **变量数（列数）**：`17` 包括临床症状、人口学特征、功能状态和代谢指标等变量。\n\n### 📊 变量列表与类型\n\n| 变量名                    | 类型     | 描述                        |\n|---------------------------|----------|-----------------------------|\n| `Age`                     | 连续变量 | 年龄（18–64岁）             |\n| `Sex_Female`              | 二元变量 | 性别（1=女性，0=男性）      |\n| `Education`               | 分类变量 | 教育水平（3类）             |\n| `Occupation_Employed`     | 二元变量 | 是否就业（1=在职）          |\n| `Duration_of_Illness`     | 连续变量 | 病程（0–44年）              |\n| `Obesity`                 | 二元变量 | 是否肥胖                    |\n| `Hypertension`            | 二元变量 | 是否高血压                  |\n| `Diabetes`                | 二元变量 | 是否糖尿病                  |\n| `CGI_Positive`            | 连续变量 | 阳性症状评分（1–7）         |\n| `CGI_Negative`            | 连续变量 | 阴性症状评分（1–7）         |\n| `CGI_Depressive`          | 连续变量 | 抑郁症状评分（1–6）         |\n| `CGI_Cognitive`           | 连续变量 | 认知缺陷评分（1–6）         |\n| `SCL90R_Somatization`     | 连续变量 | 躯体化评分（12–58）         |\n| `SCL90R_OCS`              | 连续变量 | 强迫症状评分（10–50）       |\n| `SWN_Total_Score`         | 连续变量 | 主观幸福感评分（24–120）    |\n| `DIEPSS_Overall_Severity` | 连续变量 | 锥体外系不良反应评分（0–4） |\n| `PSP_Total_Score`         | 连续变量 | 社会功能评分（10–90）       |\n\n### 📊 变量探索(机器学习进行变量的探索，即特征工程)\n\n------------------------------------------------------------------------\n\n**论文的首要分析步骤是利用机器学习 (ML) 来识别决定高水平主观幸福感 (SWB) 的最重要因素。论文作者训练了多个模型，发现随机森林 (RF) 模型表现最佳**\n\n1.  准备机器学习的数据 。\n\n2.  训练随机森林模型来预测哪些患者具有“高水平的 SWB”。\n\n3.  使用 AUC 指标评估模型的性能 ，就像论文中所做的那样。\n\n4.  使用 SHAP 值分析**特征重要性**， 以了解哪些变量是最强大的预测因子，重现论文中图 1B 中的发现。\n\n#### 第一步导入需要的包🧉\n\n-   包是可以认为是游戏的mod,都是大佬收集的算法或者其他的代码来实现你需要的功能，这些都是地基\n\n::: {#8067cc86 .cell execution_count=2}\n``` {.python .cell-code}\n# 导入所需的库\nimport pandas as pd                 # 导入 pandas 用于读取和处理数据\nimport numpy as np                  # 导入 numpy 用于数值计算\nfrom sklearn.model_selection import train_test_split # 导入 train_test_split 用于划分训练集和测试集\nfrom sklearn.ensemble import RandomForestClassifier  # 导入随机森林分类器\nfrom sklearn.metrics import roc_auc_score, roc_curve # 导入评估指标计算函数\nimport matplotlib.pyplot as plt     # 导入 matplotlib 用于绘图\nimport shap                         # 导入 SHAP 库用于模型解释\n```\n:::\n\n\n#### 第二步导入数据🤤\n\n就是我们要的数据，即调查的数据，一般是XLSX(excel表格)，csv(数据格式万金油)，或者其他格式\n\n::: {#c464736a .cell execution_count=3}\n``` {.python .cell-code}\n# --- 1. 加载并准备数据 ---\n# Load and Prepare the Data\n\n# !!! 重要提示: 请将下面的路径修改为您保存 CSV 文件的实际位置 !!!\n# For example: \"C:/Users/YourUsername/Documents/synthetic_schizophrenia_data.csv\"\nfile_path = \"./synthetic_schizophrenia_data.csv\"\n\n# 读取我们之前生成的合成数据\ndf = pd.read_csv(file_path)\n```\n:::\n\n\n#### 第三步 定义目标变量和特征变量🥶\n\n我们必须要知道what the fuck is our 目标变量，即我们要研究的目标，这篇文章研究的是 **高水平主观幸福感 (SWB)**，他们想要知道什么因素影响这个鬼东西，而这些因素，比如年龄，身高，体重就是特征变量\n\n-   **定义目标**： 本文机器学习模型的主要目标并非预测准确的主观幸福感 (SWB) 评分，而是将患者分为两组：具有“高水平 SWB”的患者和不具有“高水平 SWB”的患者。我们遵循本文的思路，创建了一个二元目标变量 ( High_SWB )，其中 80 分及以上被视为“高”（编码为 1），低于 80 分则被视为“低”（编码为 0）。\n\n::: {#54e02885 .cell execution_count=4}\n``` {.python .cell-code}\n# 定义目标变量 (Y)\n# 根据论文，\"高水平SWB\" 定义为 SWN 总分达到或超过 80\n# 我们创建一个名为 'High_SWB' 的新列，1 表示高SWB，0 表示非高SWB\ndf['High_SWB'] = (df['SWN_Total_Score'] >= 80).astype(int)\n\n# 定义特征变量 (X)\n# X 是除了目标变量和原始SWN分数之外的所有列\n# We drop the original score and the target variable to create our feature set\nX = df.drop(['SWN_Total_Score', 'High_SWB'], axis=1)\ny = df['High_SWB']\n```\n:::\n\n\n#### 特征工程\n\n特征工程是指从原始数据中提取有用的特征，以便于机器学习模型的训练和预测。\n\n特征工程包括以下几个方面：\n\n1.  特征选择：选择最相关的特征，以减少模型的复杂度，提高模型的泛化能力。即有的时候特征太多，且其中有无意义或者不重要的特征，这时候就需要选择最相关的特征，以减少模型的复杂度，提高模型的泛化能力。\n2.  特征提取：从原始数据中提取有用的特征，以便于机器学习模型的训练和预测。\n3.  特征变换：对特征进行变换，以便于机器学习模型的训练和预测。比如标准化，归一化，独热编码，等等。 对于一些其他数据类型，比如字符型，如是否为男或者女性，一般可以用1代表男，0代表女，因为模型一般无法了解字的真实含义，但是数字可以。 一般来说，对于特征工程这个步骤需要花费很多时间。我们这个例子的数据比较符合机器学习的类型，不需要太多更改\n\n独热编码是将分类变量转换为二进制变量，即将每个分类变量转换为多个二进制变量，每个二进制变量表示一个分类。比如这个例子教育是分为三类，且都为字符型，我们可以把教育分为三个二进制（0/1）变量，每个二进制变量表示一个分类。\n\n::: {#92cf4c47 .cell execution_count=5}\n``` {.python .cell-code}\n# 处理分类变量\n# 'Education' 列是文本类型，机器学习模型需要数值输入。\n# 我们使用 \"独热编码\" (One-Hot Encoding) 将其转换为数值列。\n# This converts the 'Education' column into multiple binary (0/1) columns.\nX = pd.get_dummies(X, columns=['Education'], drop_first=True)\n```\n:::\n\n\n接下来我们要进行数据标准化，他的统计学意义我就不说了，但是为什么要进行标准化，标准化可以会将所有特征缩放到均值为0，标准差为1的范围，对于逻辑回归等线性模型很重要，而对于一些随机森林，神经网络等等的黑箱模型不太重要，但是我还是进行标准化。\n\n::: {#8125d561 .cell execution_count=6}\n``` {.python .cell-code}\n# 数据标准化 (对于逻辑回归等线性模型很重要)\n# StandardScaler 会将所有特征缩放到均值为0，标准差为1的范围\n#先导入我们需要的包\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n```\n:::\n\n\n\\#### 机器学习 我们先定义一堆我们要用的机械学习的模型，论文里面用的是随机森林，逻辑回归，支持向量机，等等\n\n我们将使用五种机器学习模型来预测患者是否具有高水平的主观幸福感（SWB）。每种模型都有不同的特点和适用场景。\n\n##### 1️⃣ Logistic Regression (LR)\n\n. 逻辑回归是一种经典的线性模型，适用于二分类问题（如：高SWB vs 低SWB）。\n\n. 优点：简单、易解释、计算快。\n\n. 原理：根据每个变量的权重，计算一个概率值（0到1之间），判断属于哪个类别。\n\n. 适用场景：变量之间关系较简单、线性时效果好。\n\n2️⃣ Random Forest (RF) . 随机森林是一种集成学习方法，由多个决策树组成。\n\n. 优点：抗过拟合、能处理非线性关系、对缺失值和异常值鲁棒。\n\n. 原理：每棵树都在不同的数据子集上训练，最终通过“投票”决定预测结果。\n\n. 适用场景：变量多、关系复杂时表现优异。\n\n. 3️⃣ Gradient Boosting Machine (GBM) 梯度提升机是一种逐步优化的集成方法。\n\n. 优点：预测精度高，适合复杂数据结构。\n\n. 原理：每棵新树都专注于纠正前一棵树的错误，逐步提升整体性能。\n\n. 适用场景：需要高精度预测时，GBM是常用选择。\n\n.\n\n4️⃣ XGBoost (XGB) . 极端梯度提升是GBM的高效版本，在数据科学竞赛中非常流行。\n\n. 优点：速度快、性能强、支持缺失值处理和正则化。\n\n. 原理：在GBM基础上加入更多优化策略，如剪枝、并行计算。\n\n. 适用场景：大数据、高维度、需要快速训练时。\n\n5️⃣ LightGBM (LGBM) . 轻量级梯度提升机是另一种高效的GBM变体，专为大规模数据设计。\n\n. 优点：训练速度快、内存占用低、适合高维稀疏数据。\n\n. 原理：使用“叶子优先”策略构建树，提升效率。\n\n. 适用场景：数据量大、变量多时非常适合。\n\n**当然，随着现在算法的升级，还有很多算法在他们之上有更好的表现，也要根据数据和模型的用途进行匹配**\n\n📊 为什么要比较多个模型？ 不同模型对数据的敏感性不同，适合的场景也不同。\n\n比较多个模型可以找到最稳定、最准确的预测方法。\n\n::: {#11f8466c .cell execution_count=7}\n``` {.python .cell-code}\n#  定义要比较的模型 ---\n# 依旧导入需要的包\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# 我们将所有模型储存在一个字典中，方便循环调用\nmodels = {\n    \"Logistic Regression (LR)\": LogisticRegression(max_iter=1000, random_state=42),\n    \"Random Forest (RF)\": RandomForestClassifier(n_estimators=500, max_depth=5, random_state=42),\n    \"Gradient Boosting (GBM)\": GradientBoostingClassifier(n_estimators=200, max_depth=3, random_state=42),\n    \"XGBoost (XGB)\": xgb.XGBClassifier(n_estimators=200, max_depth=3, use_label_encoder=False, eval_metric='logloss', random_state=42),\n    \"LightGBM (LGBM)\": lgb.LGBMClassifier(n_estimators=200, max_depth=3, random_state=42, verbosity=-1)\n\n}\n```\n:::\n\n\n##### 🧪 使用 5 折交叉验证评估所有模型\n\n在这一部分，我们将对五种机器学习模型进行性能评估，使用的是一种非常稳健的方法：5 折交叉验证（5-fold cross-validation）。\n\n. ✅ 什么是交叉验证？ 交叉验证是一种将数据集分成多个“子集”来训练和测试模型的方法。\n\n. 在 5 折交叉验证中，我们将数据分成 5 份：\n\n. 每次用其中 4 份训练模型，剩下 1 份测试模型。\n\n. 这个过程重复 5 次，每次测试集都不同。\n\n. 最后我们计算 5 次的平均性能指标（如 AUC），得到一个更稳定的评估结果。\n\n. 这种方法比单次训练/测试划分更可靠，能减少“运气好/坏”带来的误差。\n\n. 📊 为什么选择 AUC？ . AUC 是衡量模型区分能力的指标，值越接近 1，模型越好。\n\n::: {#116d2e10 .cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.model_selection import cross_val_score\n\nresults = {} # 创建一个空字典来储存结果\n\nprint(\"正在使用5折交叉验证评估每个模型...\")\n\n# 循环遍历我们定义的每个模型\nfor model_name, model in models.items():\n    # 对线性模型（逻辑回归）使用标准化后的数据\n    if model_name == \"Logistic Regression (LR)\":\n        X_input = X_scaled\n    else: # 对树模型使用原始编码后的数据\n        X_input = X\n\n    # 使用 cross_val_score 函数进行5折交叉验证\n    # cv=5: 指定了折数\n    # scoring='roc_auc': 指定评估指标为 AUC\n\n    cv_scores = cross_val_score(model, X_input, y, cv=5, scoring='roc_auc')\n\n    # 将每个模型的名称和其5次交叉验证的平均分及标准差存入结果字典\n    results[model_name] = {\n        'mean_auc': np.mean(cv_scores),\n        'std_auc': np.std(cv_scores)\n    }\n    \n    # 打印每个模型的评估结果\n    print(f\"  {model_name}: 平均 AUC = {results[model_name]['mean_auc']:.4f} (标准差 = {results[model_name]['std_auc']:.4f})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n正在使用5折交叉验证评估每个模型...\n  Logistic Regression (LR): 平均 AUC = 0.7916 (标准差 = 0.0327)\n  Random Forest (RF): 平均 AUC = 0.7977 (标准差 = 0.0466)\n  Gradient Boosting (GBM): 平均 AUC = 0.7571 (标准差 = 0.0341)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nd:\\pythontext\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning:\n\n[12:04:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \nParameters: { \"use_label_encoder\" } are not used.\n\n\nd:\\pythontext\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning:\n\n[12:04:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \nParameters: { \"use_label_encoder\" } are not used.\n\n\nd:\\pythontext\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning:\n\n[12:04:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \nParameters: { \"use_label_encoder\" } are not used.\n\n\nd:\\pythontext\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning:\n\n[12:04:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \nParameters: { \"use_label_encoder\" } are not used.\n\n\nd:\\pythontext\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning:\n\n[12:04:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \nParameters: { \"use_label_encoder\" } are not used.\n\n\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n  XGBoost (XGB): 平均 AUC = 0.7572 (标准差 = 0.0422)\n  LightGBM (LGBM): 平均 AUC = 0.7609 (标准差 = 0.0444)\n```\n:::\n:::\n\n\n这些 LightGBM 的日志信息和警告是模型训练过程中的正常输出，尤其是这句：\n\n> **\\[Warning\\] No further splits with positive gain, best gain: -inf**\n\n它的意思是：在某一轮训练中，LightGBM 没有找到任何可以继续分裂的节点（即没有“正增益”的分裂点），所以该轮的树构建就提前终止了。这通常是因为：\n\n-   数据已经被很好地拟合；\n\n-   当前参数限制了树的复杂度（如 `max_depth`、`min_data_in_leaf` 等）；\n\n-   或者数据本身不支持进一步分裂（例如样本太少或特征太弱）。\n\n这不是错误，也不会影响模型的正常训练和预测性能。\n\n#### 可视化结果🤨\n\n. 比较平均 AUC 分数： 查看打印的输出和条形图。您将看到五个模型的平均 AUC 分数。 与原始论文一致，您可能会发现所有高级模型（RF，GBM，XGB，LGBM）的表现非常相似，并且明显优于基线 Logistic 回归模型 。\n\n随机森林模型可能仍然略占优势，就像论文中提到的那样，AUC 得分在 0.79-0.80 左右 。这证实了我们之前的发现，但现在的置信度更高了。顶级模型之间性能接近，这是机器学习中一个非常常见且重要的发现。\n\n标准差（误差线）的重要性： 标准差（由条形图上的黑色“误差线”表示）告诉您模型的性能在 5 个不同折叠之间的差异有多大。 标准差越小（误差线越短），效果越好。 这意味着模型稳定，其性能不会严重依赖于训练时所用的特定数据子集。\n\n您可能会注意到基于树的模型具有相对较低的方差，这表明它们很稳定。\n\n::: {#2c5c59e9 .cell execution_count=9}\n``` {.python .cell-code}\n# 将结果字典转换为 DataFrame，方便绘图\nresults_df = pd.DataFrame(results).T.sort_values(by='mean_auc', ascending=False)\n\nplt.figure(figsize=(12, 7))\nbars = plt.bar(results_df.index, results_df['mean_auc'], yerr=results_df['std_auc'], capsize=5, color='skyblue')\n\n# 在每个柱状图上显示数值\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f'{yval:.3f}', ha='center', va='bottom')\n\nplt.ylabel('Mean AUC Score ')\nplt.xlabel('Machine Learning Models ')\nplt.title('Comparison of ML Models using 5-Fold Cross-Validation ')\nplt.ylim(0, 1.0) # 设置y轴范围为0到1\nplt.xticks(rotation=15) # 旋转x轴标签，防止重叠\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=961 height=640}\n:::\n:::\n\n\n**🎯 模型表现排名（按 AUC）**\n\n| 模型名称                    | 平均 AUC 分数 |\n|-----------------------------|---------------|\n| 🥇 **Random Forest (RF)**   | **0.798**     |\n| 🥈 Logistic Regression (LR) | 0.792         |\n| 🥉 LightGBM (LGBM)          | 0.760         |\n| XGBoost (XGB)               | 0.758         |\n| Gradient Boosting (GBM)     | 0.757         |\n\n**📌 评估**\n\n> 在这次评估中，**Random Forest** 不仅在平均 AUC 上领先，而且其稳定性也很高（标准差较小）。这说明它在不同数据划分下都能保持良好的预测能力，是一个非常可靠的模型选择。\n\n-   虽然逻辑回归易于解释，但在复杂数据中，集成树模型（如 RF）往往能捕捉更丰富的非线性关系；\n\n-   RF 的优势在于“稳健性”和“泛化能力”，尤其适合临床数据这种变量多、结构复杂的场景；\n\n-   这也验证了原始论文的结论：RF 是预测主观幸福感的首选模型之一。\n\n------------------------------------------------------------------------\n\n## 改进模型🤖🤖\n\n虽然我们的模型AUC有0.8左右，这样的数据已经不错了，如果比较原来的论文[原论文链接](https://pmc.ncbi.nlm.nih.gov/articles/PMC12236327/#s5)\n\n`The random forest (RF) model had the highest area under the curve (AUC) of 0.794 at baseline. Obsessive-compulsive symptoms (OCS) had the most significant impact on high levels of SWB, followed by somatization, cognitive deficits, and depression. The network analysis demonstrated robust connections among the SWB, OCS, and somatization. SEM analysis revealed that OCS exerted the strongest direct effect on SWB, and also an indirect effect via the mediation of depression. Furthermore, the contribution of OCS at baseline to SWB was maintained 6 months later.随机森林 (RF) 模型的基线曲线下面积 (AUC) 最高，为 0.794。强迫症状 (OCS) 对高水平主观幸福感 (SWB) 的影响最为显著，其次是躯体化、认知缺陷和抑郁症。网络分析表明，SWB、OCS 和躯体化之间存在稳态联系。结构方程模型 (SEM) 分析显示，OCS 对 SWB 的直接影响最为显著，并且也通过抑郁症的中介产生间接影响。此外，基线 OCS 对 SWB 的贡献在 6 个月后得以维持。`\n\n看起来确实已经不错了，但是我们还是可以对原来的模型进行改善，这是原论文没有进行的操作，具体有两种方法，在模型不改变的情况下\n\n**特征选择（Feature Selection）** 和 **超参数调整（Hyperparameter Tuning）**\n\n#### 🧠 为什么要进行特征选择？\n\n> **理念：少即是多（Less is More）**\n\n在机器学习中，使用所有变量并不总是最好的选择。有些变量可能：\n\n-   与目标无关（噪声）；\n\n-   与其他变量高度冗余（重复信息）；\n\n-   增加模型复杂度，导致过拟合；\n\n-   降低模型解释性，让临床人员难以理解。\n\n### ✅ 特征选择的好处：\n\n-   **提升模型性能**：去除无关变量后，模型更专注于有效信息；\n\n-   **提高训练速度**：变量少，计算更快；\n\n-   **增强解释性**：更容易向临床人员解释模型决策；\n\n-   **减少过拟合风险**：模型更稳健，泛化能力更强。\n\n#### 🔧 为什么要进行超参数调整？\n\n> **理念：模型的“个性化设置”决定它的表现**\n\n每个机器学习模型都有一些“内部设置”，称为**超参数**（hyperparameters），它们不是从数据中学出来的，而是你提前设定的。\n\n例如：\n\n-   随机森林的 `n_estimators`（树的数量）、`max_depth`（树的最大深度）；\n\n-   XGBoost 的 `learning_rate`（学习率）、`subsample`（样本采样比例）；\n\n-   LightGBM 的 `num_leaves`（叶子数）、`min_data_in_leaf`（每个叶子最少样本数）。\n\n### ✅ 超参数调整的好处：\n\n-   **提升模型性能**：找到最适合当前数据的参数组合；\n\n-   **控制模型复杂度**：防止过拟合或欠拟合；\n\n-   **增强模型稳定性**：提高在不同数据划分下的表现一致性。\n\n### 特征选择🤩\n\n::: {#47fdf8bb .cell execution_count=10}\n``` {.python .cell-code}\n# 导入所需的库\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\n# --- 2. 识别最重要的特征 ---\n# Identify the most important features\n\n#和以前一样导入数据\n# --- 1. 加载并准备数据 (与之前相同) ---\n# Load and Prepare the Data\n\n# !!! 重要提示: 请将下面的路径修改为您保存 CSV 文件的实际位置 !!!\nfile_path = \"./synthetic_schizophrenia_data.csv\"\ndf = pd.read_csv(file_path)\n\ndf['High_SWB'] = (df['SWN_Total_Score'] >= 80).astype(int)\nX = df.drop(['SWN_Total_Score', 'High_SWB'], axis=1)\ny = df['High_SWB']\nX = pd.get_dummies(X, columns=['Education'], drop_first=True)\n\n\n\n\nprint(\"--- 方法一: 特征选择 ---\")\nprint(\"首先，我们训练一个基础模型来获取特征重要性...\")\n\n# 训练一个临时的随机森林模型来获取特征重要性排序\ntemp_rf = RandomForestClassifier(n_estimators=500, max_depth=5, random_state=42)\ntemp_rf.fit(X, y)\n\n# 创建一个包含特征名称和其重要性分数的 DataFrame\nfeature_importances = pd.DataFrame({\n    'feature': X.columns,\n    'importance': temp_rf.feature_importances_\n}).sort_values('importance', ascending=False)\n\n# 打印最重要的特征\nprint(\"\\n根据基础模型得出的特征重要性排序:\")\nprint(feature_importances)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n--- 方法一: 特征选择 ---\n首先，我们训练一个基础模型来获取特征重要性...\n\n根据基础模型得出的特征重要性排序:\n                         feature  importance\n12                    SCL90R_OCS    0.303654\n9                 CGI_Depressive    0.153773\n11           SCL90R_Somatization    0.127664\n14               PSP_Total_Score    0.098338\n8                   CGI_Negative    0.057625\n3            Duration_of_Illness    0.050011\n7                   CGI_Positive    0.049062\n10                 CGI_Cognitive    0.048823\n0                            Age    0.040797\n13       DIEPSS_Overall_Severity    0.026666\n4                        Obesity    0.009579\n6                       Diabetes    0.007630\n2            Occupation_Employed    0.006329\n15         Education_High school    0.006138\n5                   Hypertension    0.004924\n1                     Sex_Female    0.004714\n16  Education_Less than 12 years    0.004275\n```\n:::\n:::\n\n\n#### 📊 删除建议（按重要性断层）\n\n我打算删除的变量，其重要性如下：\n\n| 特征名称                     | 重要性 |\n|------------------------------|--------|\n| DIEPSS_Overall_Severity      | 0.0267 |\n| Obesity                      | 0.0096 |\n| Diabetes                     | 0.0076 |\n| Occupation_Employed          | 0.0063 |\n| Education_High school        | 0.0061 |\n| Hypertension                 | 0.0049 |\n| Sex_Female                   | 0.0047 |\n| Education_Less than 12 years | 0.0043 |\n\n这些变量都和 `Age` 的重要性（0.0408）断层，说明它们可能对模型预测贡献非常有限。\n\n但是考虑到`DIEPSS_Oerall_Severity`对于实际意义的确有影响，且和其后面的也断层了，所以我打算保留\n\n***特征选择不仅是“数学问题”，也是“领域知识问题”。***\n\n所以我们已经选出了***10个贡献比较大的特征***\n\n::: {#750d8994 .cell execution_count=11}\n``` {.python .cell-code}\n# 选择最重要的前 10 个特征\nN_TOP_FEATURES = 10\ntop_features = feature_importances['feature'].head(N_TOP_FEATURES).tolist()\nprint(f\"\\n我们将选择最重要的 {N_TOP_FEATURES} 个特征进行建模: {top_features}\")\n\n# 创建只包含最重要特征的新数据集\nX_selected = X[top_features]\n\n# --- 3. 在筛选后的特征上重新评估模型 ---\n# Re-evaluate the model on selected features\n\nprint(\"\\n使用筛选出的特征进行5折交叉验证...\")\n# 使用与之前相同的模型配置\nmodel_on_selected_features = RandomForestClassifier(n_estimators=500, max_depth=5, random_state=42)\ncv_scores_selected = cross_val_score(model_on_selected_features, X_selected, y, cv=5, scoring='roc_auc')\n\n# 打印结果\nmean_auc_selected = np.mean(cv_scores_selected)\nprint(f\"\\n使用最重要的 {N_TOP_FEATURES} 个特征, 模型的平均 AUC 分数是: {mean_auc_selected:.4f}\")\nprint(f\"原始模型的平均 AUC 分数是: 0.7980 (作为对比)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n我们将选择最重要的 10 个特征进行建模: ['SCL90R_OCS', 'CGI_Depressive', 'SCL90R_Somatization', 'PSP_Total_Score', 'CGI_Negative', 'Duration_of_Illness', 'CGI_Positive', 'CGI_Cognitive', 'Age', 'DIEPSS_Overall_Severity']\n\n使用筛选出的特征进行5折交叉验证...\n\n使用最重要的 10 个特征, 模型的平均 AUC 分数是: 0.7991\n原始模型的平均 AUC 分数是: 0.7980 (作为对比)\n```\n:::\n:::\n\n\n为了加以区别，我们分别复制代码，选取9个和11个特征进行训练看看有什么区别，即剔除`DIEPSS_Overall_Severity 0.026666` 和加上不太重要的`Obesity 0.009579`的变量\n\n::: {#3af3dcea .cell execution_count=12}\n``` {.python .cell-code}\n# 选择最重要的前 9 个特征\n#剔除DIEPSS_Overall_Severity 0.026666\nN_TOP_FEATURES = 9\ntop_features = feature_importances['feature'].head(N_TOP_FEATURES).tolist()\nprint(f\"\\n我们将选择最重要的 {N_TOP_FEATURES} 个特征进行建模: {top_features}\")\n\n# 创建只包含最重要特征的新数据集\nX_selected = X[top_features]\n\n# --- 3. 在筛选后的特征上重新评估模型 ---\n# Re-evaluate the model on selected features\n\nprint(\"\\n使用筛选出的特征进行5折交叉验证...\")\n# 使用与之前相同的模型配置\nmodel_on_selected_features = RandomForestClassifier(n_estimators=500, max_depth=5, random_state=42)\ncv_scores_selected = cross_val_score(model_on_selected_features, X_selected, y, cv=5, scoring='roc_auc' )\n\n# 打印结果\nmean_auc_selected = np.mean(cv_scores_selected)\nprint(f\"\\n使用最重要的 {N_TOP_FEATURES} 个特征, 模型的平均 AUC 分数是: {mean_auc_selected:.4f}\")\nprint(f\"原始模型的平均 AUC 分数是: 0.7980 (作为对比)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n我们将选择最重要的 9 个特征进行建模: ['SCL90R_OCS', 'CGI_Depressive', 'SCL90R_Somatization', 'PSP_Total_Score', 'CGI_Negative', 'Duration_of_Illness', 'CGI_Positive', 'CGI_Cognitive', 'Age']\n\n使用筛选出的特征进行5折交叉验证...\n\n使用最重要的 9 个特征, 模型的平均 AUC 分数是: 0.7975\n原始模型的平均 AUC 分数是: 0.7980 (作为对比)\n```\n:::\n:::\n\n\n::: {#ec1bed1e .cell execution_count=13}\n``` {.python .cell-code}\n# 选择最重要的前 11 个特征\n# 加上不太重要的Obesity 0.009579的变量\nN_TOP_FEATURES = 11\ntop_features = feature_importances['feature'].head(N_TOP_FEATURES).tolist()\nprint(f\"\\n我们将选择最重要的 {N_TOP_FEATURES} 个特征进行建模: {top_features}\")\n\n# 创建只包含最重要特征的新数据集\nX_selected = X[top_features]\n\n# --- 3. 在筛选后的特征上重新评估模型 ---\n# Re-evaluate the model on selected features\n\nprint(\"\\n使用筛选出的特征进行5折交叉验证...\")\n# 使用与之前相同的模型配置\nmodel_on_selected_features = RandomForestClassifier(n_estimators=500, max_depth=5, random_state=42)\ncv_scores_selected = cross_val_score(model_on_selected_features, X_selected, y, cv=5, scoring='roc_auc')\n\n# 打印结果\nmean_auc_selected = np.mean(cv_scores_selected)\nprint(f\"\\n使用最重要的 {N_TOP_FEATURES} 个特征, 模型的平均 AUC 分数是: {mean_auc_selected:.4f}\")\nprint(f\"原始模型的平均 AUC 分数是: 0.7980 (作为对比)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n我们将选择最重要的 11 个特征进行建模: ['SCL90R_OCS', 'CGI_Depressive', 'SCL90R_Somatization', 'PSP_Total_Score', 'CGI_Negative', 'Duration_of_Illness', 'CGI_Positive', 'CGI_Cognitive', 'Age', 'DIEPSS_Overall_Severity', 'Obesity']\n\n使用筛选出的特征进行5折交叉验证...\n\n使用最重要的 11 个特征, 模型的平均 AUC 分数是: 0.7974\n原始模型的平均 AUC 分数是: 0.7980 (作为对比)\n```\n:::\n:::\n\n\n#### 🎯 教学总结：特征选择的实证价值\n\n| 特征数量                           | 平均 AUC 分数 | 相对表现    |\n|------------------------------------|---------------|-------------|\n| 原始模型（全部特征）               | 0.7980        | 基准        |\n| 最重要的 10 个特征                 | **0.7991**    | ✅ 最佳表现 |\n| 最重要的 9 个特征（剔除 DIEPSS）   | 0.7975        | 略低        |\n| 最重要的 11 个特征（加入 Obesity） | 0.7974        | 略低        |\n\n#### 🧠 要点一：不是特征越多越好\n\n> 增加变量并不一定提升模型性能，反而可能引入噪声或冗余信息。\n\n-   加入 **Obesity（0.009579）** 后，模型性能略微下降；\n\n-   删除 **DIEPSS（0.026666）** 后，模型也略微下降；\n\n-   说明这两个变量虽然贡献不大，但也不是完全无效，尤其 DIEPSS 可能有一定临床解释价值。\n\n#### 🧠 要点二：找到“最佳变量组合”才是关键\n\n> 10个变量组合的模型表现最好，说明它在信息量与简洁性之间达到了平衡。\n\n-   这个组合既保留了高贡献变量，又避免了低贡献变量的干扰；\n\n-   模型更快、更稳定、更易解释；。\n\n#### 🧠 要点三：进行“变量敏感性分析”\n\n-   逐个添加或剔除变量，观察模型性能变化；\n\n-   结合领域知识判断是否保留某些临床重要但模型贡献较低的变量；\n\n-   用图表展示 AUC 随变量数量变化的趋势，强化“变量选择是策略性决策”的理念。\n\n### 超参数调优🎃\n\n例如，假设 n_estimators=500，max_depth=5，但这些设置可能并非我们特定“赛道”（数据集）的最佳设置。通过系统地尝试这些设置的不同组合，我们可以找到能够实现最佳性能的配置 \\#### 先是网格搜索\n\n手动定义一个固定的 param_grid 然后进行“暴力”搜索（Brute-force Search），是超参数调优最基础的方法\n\n::: {#52f23765 .cell execution_count=14}\n``` {.python .cell-code}\n# 导入 GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\nprint(\"\\n\\n--- 方法二: 超参数调优 (这可能需要几分钟) ---\")\n\n# --- 1. 定义要搜索的超参数网格 ---\n# Define the parameter grid to search\n\n# 这是一个我们要测试的超参数组合的 \"菜单\"\nparam_grid = {\n    'n_estimators': [200, 400, 600],       # 树的数量\n    'max_depth': [4, 5, 6, 7],             # 树的最大深度\n    'min_samples_leaf': [1, 3, 5],         # 一个叶节点上最少的样本数\n    'max_features': ['sqrt', 'log2']       # 每次分裂时考虑的最大特征数\n}\n\n# --- 2. 设置并运行 GridSearchCV ---\n# Setup and run GridSearchCV\n\n# 初始化我们的基础模型\nrf_base = RandomForestClassifier(random_state=42)\n\n# 设置 GridSearchCV\n# estimator: 我们要调优的模型\n# param_grid: 我们定义的参数网格\n# cv=5: 使用5折交叉验证来评估每一种参数组合\n# scoring='roc_auc': 我们的评估指标\n\ngrid_search = GridSearchCV(estimator=rf_base, param_grid=param_grid, cv=5, scoring='roc_auc' , verbose=1)\n\n# 在完整数据集上运行网格搜索\ngrid_search.fit(X, y)\n\n# --- 3. 打印最佳结果 ---\n# Print the best results\n\nprint(\"\\n超参数调优完成!\")\n\n# 打印找到的最佳参数组合\nprint(f\"找到的最佳超参数组合 (Best Parameters): \\n{grid_search.best_params_}\")\n\n# 打印使用最佳参数时，交叉验证得到的最佳AUC分数\nprint(f\"\\n使用最佳参数时，模型的最佳平均 AUC 分数是: {grid_search.best_score_:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\n--- 方法二: 超参数调优 (这可能需要几分钟) ---\nFitting 5 folds for each of 72 candidates, totalling 360 fits\n\n超参数调优完成!\n找到的最佳超参数组合 (Best Parameters): \n{'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 3, 'n_estimators': 200}\n\n使用最佳参数时，模型的最佳平均 AUC 分数是: 0.7997\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nd:\\pythontext\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2846: RuntimeWarning:\n\ninvalid value encountered in cast\n\n```\n:::\n:::\n\n\n使用最佳参数时，模型的最佳平均 AUC 分数是: 0.7997 正如我说，网格搜索是超参数最基础的方法，但它有两个主要缺点：\n\n1.  **维度诅咒 (Curse of Dimensionality):** 当参数和你想尝试的值变多时，组合的数量会呈指数级增长，导致计算成本极高，非常耗时。\n\n2.  **效率低下:** 它会花费大量时间去尝试那些明显不佳的参数组合。\n\n接下来我会用一个更加爽的方法，***即贝叶斯优化***\n\n这是目前最先进和最高效的调优方法之一，它完美地体现了“迭代选择最好”的思想。\n\n**核心思想：**\\\n贝叶斯优化会建立一个关于“哪个参数组合可能得到更高分数”的概率模型。\n\n1.  它首先尝试几个随机的参数组合。\n\n2.  然后，它根据已有的结果，更新内部的概率模型，预测出“最有希望”提升分数的下一个参数组合。\n\n3.  它会去尝试这个“最有希望”的组合，然后再次更新模型。\n\n4.  这个过程会不断迭代，智能地将搜索资源集中在参数空间中最有潜力的区域。\n\n**打个比方：** 这就像在一个大雾弥漫的山区里找最高峰。你不会在地毯式搜索每一寸土地（网格搜索），而是根据你当前所在位置的高度和坡度，来判断下一步往哪个方向走最有可能登得更高。\n\n**🥸不过需要注意的是，由于是随机组合，每次远行代码得到的最好的结果都不一样，就好像在乌泱泱的大海找最大的船，有的时候你不一定能找到山东舰航母，可能只会找到护卫舰**\n\n::: {#a66050be .cell execution_count=15}\n``` {.python .cell-code}\n# 导入 optuna 库\nimport optuna\nfrom sklearn.model_selection import cross_val_score\n\nprint(\"\\n\\n--- 最智能的调优方法二: 贝叶斯优化 (Bayesian Optimization with Optuna) ---\")\nprint(\"Optuna 会根据历史结果，智能地选择下一次要尝试的超参数...\")\n\n# --- 1. 定义一个 \"目标函数\" (Objective Function) ---\n# 这个函数告诉 Optuna 如何评估一组超参数的好坏\ndef objective(trial):\n    # 'trial' 对象用于建议超参数的值\n    # Optuna 会智能地决定建议值的范围和具体值\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 200, 800), # 建议一个在 [200, 800] 范围内的整数\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n    }\n    \n    # 使用这组参数创建并评估模型\n    model = RandomForestClassifier(random_state=42, **params)\n    \n    # 使用交叉验证来获得稳健的分数\n    score = cross_val_score(model, X, y, cv=5, scoring='roc_auc', ).mean()\n    \n    # Optuna 的目标是最大化这个返回值\n    return score\n\n# --- 2. 创建一个 \"研究\" (Study) 并开始优化 ---\n# direction='maximize': 告诉 Optuna 我们的目标是最大化 objective 函数的返回值\nstudy = optuna.create_study(direction='maximize', study_name='RF_Optimization')\n\n# n_trials=100: 运行100次迭代。Optuna 会在这100次中智能地寻找最优解。\nstudy.optimize(objective, n_trials=100)\n\n# --- 3. 打印最佳结果 ---\nprint(\"\\n贝叶斯优化完成!\")\nprint(f\"迭代次数 (Number of finished trials): {len(study.trials)}\")\nprint(f\"找到的最佳超参数组合 (Best Parameters): \\n{study.best_params}\")\nprint(f\"\\n使用最佳参数时，模型的最佳平均 AUC 分数是: {study.best_value:.4f}\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n[I 2025-09-12 12:06:24,092] A new study created in memory with name: RF_Optimization\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\n--- 最智能的调优方法二: 贝叶斯优化 (Bayesian Optimization with Optuna) ---\nOptuna 会根据历史结果，智能地选择下一次要尝试的超参数...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n[I 2025-09-12 12:06:26,062] Trial 0 finished with value: 0.7975024428376002 and parameters: {'n_estimators': 662, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.7975024428376002.\n[I 2025-09-12 12:06:28,401] Trial 1 finished with value: 0.7987354124860382 and parameters: {'n_estimators': 758, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 1 with value: 0.7987354124860382.\n[I 2025-09-12 12:06:30,359] Trial 2 finished with value: 0.7962259708194112 and parameters: {'n_estimators': 547, 'max_depth': 6, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 1 with value: 0.7987354124860382.\n[I 2025-09-12 12:06:32,669] Trial 3 finished with value: 0.7951182333398477 and parameters: {'n_estimators': 588, 'max_depth': 9, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 1 with value: 0.7987354124860382.\n[I 2025-09-12 12:06:34,517] Trial 4 finished with value: 0.795039522331905 and parameters: {'n_estimators': 507, 'max_depth': 10, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 1 with value: 0.7987354124860382.\n[I 2025-09-12 12:06:35,368] Trial 5 finished with value: 0.7947395657071677 and parameters: {'n_estimators': 226, 'max_depth': 8, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 1 with value: 0.7987354124860382.\n[I 2025-09-12 12:06:38,438] Trial 6 finished with value: 0.7902455071648625 and parameters: {'n_estimators': 786, 'max_depth': 9, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7987354124860382.\n[I 2025-09-12 12:06:40,595] Trial 7 finished with value: 0.7979688301866407 and parameters: {'n_estimators': 642, 'max_depth': 5, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 1 with value: 0.7987354124860382.\n[I 2025-09-12 12:06:41,723] Trial 8 finished with value: 0.7974651369339568 and parameters: {'n_estimators': 334, 'max_depth': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7987354124860382.\n[I 2025-09-12 12:06:43,010] Trial 9 finished with value: 0.796392687534458 and parameters: {'n_estimators': 358, 'max_depth': 6, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.7987354124860382.\n[I 2025-09-12 12:06:45,395] Trial 10 finished with value: 0.7988974262421928 and parameters: {'n_estimators': 794, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 10 with value: 0.7988974262421928.\n[I 2025-09-12 12:06:47,759] Trial 11 finished with value: 0.7989554982514845 and parameters: {'n_estimators': 780, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 11 with value: 0.7989554982514845.\n[I 2025-09-12 12:06:50,104] Trial 12 finished with value: 0.7962377123282666 and parameters: {'n_estimators': 718, 'max_depth': 4, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 11 with value: 0.7989554982514845.\n[I 2025-09-12 12:06:52,505] Trial 13 finished with value: 0.7991797308827104 and parameters: {'n_estimators': 784, 'max_depth': 3, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:06:54,654] Trial 14 finished with value: 0.7985183455117725 and parameters: {'n_estimators': 676, 'max_depth': 4, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:06:56,302] Trial 15 finished with value: 0.7961692652103383 and parameters: {'n_estimators': 433, 'max_depth': 7, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:06:58,239] Trial 16 finished with value: 0.7980679736443641 and parameters: {'n_estimators': 594, 'max_depth': 4, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:00,746] Trial 17 finished with value: 0.7981055178736269 and parameters: {'n_estimators': 733, 'max_depth': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:02,854] Trial 18 finished with value: 0.7987934368302058 and parameters: {'n_estimators': 708, 'max_depth': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:04,519] Trial 19 finished with value: 0.7927481803838949 and parameters: {'n_estimators': 446, 'max_depth': 7, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:06,439] Trial 20 finished with value: 0.7965123746605846 and parameters: {'n_estimators': 602, 'max_depth': 4, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:08,722] Trial 21 finished with value: 0.7984538545991284 and parameters: {'n_estimators': 772, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:11,054] Trial 22 finished with value: 0.7986725898527307 and parameters: {'n_estimators': 796, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:13,320] Trial 23 finished with value: 0.7970167670017525 and parameters: {'n_estimators': 699, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:16,033] Trial 24 finished with value: 0.7969568360526286 and parameters: {'n_estimators': 796, 'max_depth': 5, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:17,950] Trial 25 finished with value: 0.797897348389157 and parameters: {'n_estimators': 647, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:20,310] Trial 26 finished with value: 0.7973571436515616 and parameters: {'n_estimators': 741, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:22,519] Trial 27 finished with value: 0.7985077797426401 and parameters: {'n_estimators': 750, 'max_depth': 3, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:24,958] Trial 28 finished with value: 0.7962730480734551 and parameters: {'n_estimators': 696, 'max_depth': 6, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:26,896] Trial 29 finished with value: 0.7975611186051278 and parameters: {'n_estimators': 663, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:29,090] Trial 30 finished with value: 0.7983943844128689 and parameters: {'n_estimators': 639, 'max_depth': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:31,244] Trial 31 finished with value: 0.7985672499288995 and parameters: {'n_estimators': 730, 'max_depth': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:33,451] Trial 32 finished with value: 0.7985686481058675 and parameters: {'n_estimators': 750, 'max_depth': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:36,126] Trial 33 finished with value: 0.7973512808413212 and parameters: {'n_estimators': 796, 'max_depth': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:38,171] Trial 34 finished with value: 0.7982323229915902 and parameters: {'n_estimators': 701, 'max_depth': 3, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:40,563] Trial 35 finished with value: 0.7973017250008342 and parameters: {'n_estimators': 758, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:42,078] Trial 36 finished with value: 0.7977246258684982 and parameters: {'n_estimators': 516, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:43,696] Trial 37 finished with value: 0.7977758023231981 and parameters: {'n_estimators': 550, 'max_depth': 3, 'min_samples_leaf': 5, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:46,833] Trial 38 finished with value: 0.7908430848232657 and parameters: {'n_estimators': 769, 'max_depth': 10, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:48,996] Trial 39 finished with value: 0.7970188166020804 and parameters: {'n_estimators': 683, 'max_depth': 4, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:50,032] Trial 40 finished with value: 0.7949377414039922 and parameters: {'n_estimators': 259, 'max_depth': 9, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:52,125] Trial 41 finished with value: 0.7978447260923656 and parameters: {'n_estimators': 714, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:54,374] Trial 42 finished with value: 0.7986793424119506 and parameters: {'n_estimators': 766, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:56,335] Trial 43 finished with value: 0.7959593162279092 and parameters: {'n_estimators': 617, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:07:59,169] Trial 44 finished with value: 0.7920274360453199 and parameters: {'n_estimators': 723, 'max_depth': 8, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:01,582] Trial 45 finished with value: 0.7984456561978167 and parameters: {'n_estimators': 800, 'max_depth': 3, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:04,281] Trial 46 finished with value: 0.798224410581022 and parameters: {'n_estimators': 770, 'max_depth': 5, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:06,043] Trial 47 finished with value: 0.797908803907269 and parameters: {'n_estimators': 557, 'max_depth': 4, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:08,444] Trial 48 finished with value: 0.7954266108031415 and parameters: {'n_estimators': 678, 'max_depth': 8, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:10,962] Trial 49 finished with value: 0.7985558897410353 and parameters: {'n_estimators': 739, 'max_depth': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:13,510] Trial 50 finished with value: 0.7984145944254049 and parameters: {'n_estimators': 781, 'max_depth': 4, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:15,836] Trial 51 finished with value: 0.7986233200029871 and parameters: {'n_estimators': 767, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:17,925] Trial 52 finished with value: 0.7979007485013291 and parameters: {'n_estimators': 713, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:20,189] Trial 53 finished with value: 0.7986760376300264 and parameters: {'n_estimators': 774, 'max_depth': 3, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:22,379] Trial 54 finished with value: 0.7982296696330262 and parameters: {'n_estimators': 746, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:23,807] Trial 55 finished with value: 0.7959633200983173 and parameters: {'n_estimators': 448, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:26,099] Trial 56 finished with value: 0.7989554982514845 and parameters: {'n_estimators': 780, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:27,258] Trial 57 finished with value: 0.7975624214518479 and parameters: {'n_estimators': 381, 'max_depth': 3, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:29,353] Trial 58 finished with value: 0.7961802917423351 and parameters: {'n_estimators': 658, 'max_depth': 4, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:31,678] Trial 59 finished with value: 0.797187630582579 and parameters: {'n_estimators': 735, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:34,000] Trial 60 finished with value: 0.798616567443767 and parameters: {'n_estimators': 790, 'max_depth': 3, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:36,296] Trial 61 finished with value: 0.7987354124860382 and parameters: {'n_estimators': 758, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:38,348] Trial 62 finished with value: 0.7978979998125172 and parameters: {'n_estimators': 700, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:41,160] Trial 63 finished with value: 0.7944944398632965 and parameters: {'n_estimators': 754, 'max_depth': 7, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:43,446] Trial 64 finished with value: 0.7989554982514845 and parameters: {'n_estimators': 780, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:45,962] Trial 65 finished with value: 0.7978601854808856 and parameters: {'n_estimators': 800, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:48,085] Trial 66 finished with value: 0.7982924446012097 and parameters: {'n_estimators': 723, 'max_depth': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:50,377] Trial 67 finished with value: 0.7991790317942264 and parameters: {'n_estimators': 783, 'max_depth': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:52,708] Trial 68 finished with value: 0.7991797308827104 and parameters: {'n_estimators': 784, 'max_depth': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:54,965] Trial 69 finished with value: 0.7989533533209088 and parameters: {'n_estimators': 779, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:57,231] Trial 70 finished with value: 0.7991207691244393 and parameters: {'n_estimators': 783, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:08:59,495] Trial 71 finished with value: 0.7988945822231333 and parameters: {'n_estimators': 781, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:09:01,790] Trial 72 finished with value: 0.7991228187247673 and parameters: {'n_estimators': 786, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:09:04,018] Trial 73 finished with value: 0.7979473649925086 and parameters: {'n_estimators': 744, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:09:06,283] Trial 74 finished with value: 0.7988433104381855 and parameters: {'n_estimators': 777, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:09:08,743] Trial 75 finished with value: 0.7968341619120706 and parameters: {'n_estimators': 728, 'max_depth': 6, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:09:11,133] Trial 76 finished with value: 0.7989633153318049 and parameters: {'n_estimators': 757, 'max_depth': 4, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:09:13,504] Trial 77 finished with value: 0.7990173358055644 and parameters: {'n_estimators': 754, 'max_depth': 4, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:09:16,033] Trial 78 finished with value: 0.7980208010600724 and parameters: {'n_estimators': 757, 'max_depth': 5, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:09:18,217] Trial 79 finished with value: 0.7984650241264969 and parameters: {'n_estimators': 692, 'max_depth': 4, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 13 with value: 0.7991797308827104.\n[I 2025-09-12 12:09:19,009] Trial 80 finished with value: 0.7998832839999301 and parameters: {'n_estimators': 249, 'max_depth': 4, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 80 with value: 0.7998832839999301.\n[I 2025-09-12 12:09:20,085] Trial 81 finished with value: 0.7987002832897196 and parameters: {'n_estimators': 340, 'max_depth': 4, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 80 with value: 0.7998832839999301.\n[I 2025-09-12 12:09:20,780] Trial 82 finished with value: 0.7999420074325816 and parameters: {'n_estimators': 217, 'max_depth': 4, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 82 with value: 0.7999420074325816.\n[I 2025-09-12 12:09:21,444] Trial 83 finished with value: 0.79937963841237 and parameters: {'n_estimators': 205, 'max_depth': 4, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 82 with value: 0.7999420074325816.\n[I 2025-09-12 12:09:22,218] Trial 84 finished with value: 0.8005951467370839 and parameters: {'n_estimators': 230, 'max_depth': 5, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 84 with value: 0.8005951467370839.\n[I 2025-09-12 12:09:22,923] Trial 85 finished with value: 0.8010832852710001 and parameters: {'n_estimators': 209, 'max_depth': 5, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 85 with value: 0.8010832852710001.\n[I 2025-09-12 12:09:23,606] Trial 86 finished with value: 0.8007435600445509 and parameters: {'n_estimators': 205, 'max_depth': 5, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 85 with value: 0.8010832852710001.\n[I 2025-09-12 12:09:24,289] Trial 87 finished with value: 0.8028189630928946 and parameters: {'n_estimators': 203, 'max_depth': 5, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 87 with value: 0.8028189630928946.\n[I 2025-09-12 12:09:25,005] Trial 88 finished with value: 0.8034329375539212 and parameters: {'n_estimators': 212, 'max_depth': 5, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 88 with value: 0.8034329375539212.\n[I 2025-09-12 12:09:25,691] Trial 89 finished with value: 0.8029850283845812 and parameters: {'n_estimators': 200, 'max_depth': 5, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 88 with value: 0.8034329375539212.\n[I 2025-09-12 12:09:26,379] Trial 90 finished with value: 0.8031572424772518 and parameters: {'n_estimators': 202, 'max_depth': 5, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 88 with value: 0.8034329375539212.\n[I 2025-09-12 12:09:27,084] Trial 91 finished with value: 0.8037109999984111 and parameters: {'n_estimators': 209, 'max_depth': 5, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 91 with value: 0.8037109999984111.\n[I 2025-09-12 12:09:27,963] Trial 92 finished with value: 0.7980920445319364 and parameters: {'n_estimators': 254, 'max_depth': 6, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 91 with value: 0.8037109999984111.\n[I 2025-09-12 12:09:28,780] Trial 93 finished with value: 0.8030499482833406 and parameters: {'n_estimators': 244, 'max_depth': 5, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 91 with value: 0.8037109999984111.\n[I 2025-09-12 12:09:29,726] Trial 94 finished with value: 0.8021223373070159 and parameters: {'n_estimators': 281, 'max_depth': 5, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 91 with value: 0.8037109999984111.\n[I 2025-09-12 12:09:30,681] Trial 95 finished with value: 0.8022945037345626 and parameters: {'n_estimators': 284, 'max_depth': 5, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 91 with value: 0.8037109999984111.\n[I 2025-09-12 12:09:31,655] Trial 96 finished with value: 0.8024038951939255 and parameters: {'n_estimators': 287, 'max_depth': 5, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 91 with value: 0.8037109999984111.\n[I 2025-09-12 12:09:32,636] Trial 97 finished with value: 0.8022310773430188 and parameters: {'n_estimators': 291, 'max_depth': 5, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 91 with value: 0.8037109999984111.\n[I 2025-09-12 12:09:33,649] Trial 98 finished with value: 0.7965552573837249 and parameters: {'n_estimators': 291, 'max_depth': 6, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 91 with value: 0.8037109999984111.\n[I 2025-09-12 12:09:34,614] Trial 99 finished with value: 0.802574059686268 and parameters: {'n_estimators': 286, 'max_depth': 5, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 91 with value: 0.8037109999984111.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n贝叶斯优化完成!\n迭代次数 (Number of finished trials): 100\n找到的最佳超参数组合 (Best Parameters): \n{'n_estimators': 209, 'max_depth': 5, 'min_samples_leaf': 8, 'max_features': 'sqrt'}\n\n使用最佳参数时，模型的最佳平均 AUC 分数是: 0.8037\n```\n:::\n:::\n\n\n使用最佳参数时，模型的最佳平均 AUC 分数是: 0.8042(差不多远行了好几次的最好数据),这是迭代只有100次，但是也可以多迭代，设置到200，500等等，只要你的计算机能抗住。\n\n### 最终优化：结合特征选择与贝叶斯调优🤤\n\n**目标**： 在我们已经验证过的“最佳特征集”（最重要的10个特征）上，使用贝叶斯优化（Optuna）来寻找最优的超参数组合，从而获得理论上性能最好的模型\n\n::: {#820c1fb2 .cell execution_count=16}\n``` {.python .cell-code}\n# 导入所有需要的库\nimport pandas as pd\nimport numpy as np\nimport optuna\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\n\n# --- 1. 最终步骤一：确定并筛选出最佳特征集 (Top 10) ---\n# The first step of our final process: identify and select the best feature set.\n\n# !!! 重要提示: 请将下面的路径修改为您保存 CSV 文件的实际位置 !!!\nfile_path = \"./synthetic_schizophrenia_data.csv\"\ndf = pd.read_csv(file_path)\n\ndf['High_SWB'] = (df['SWN_Total_Score'] >= 80).astype(int)\nX = df.drop(['SWN_Total_Score', 'High_SWB'], axis=1)\ny = df['High_SWB']\nX = pd.get_dummies(X, columns=['Education'], drop_first=True)\n\n\n# 训练一个临时模型来获取特征重要性\ntemp_rf = RandomForestClassifier(n_estimators=500, max_depth=5, random_state=42, )\ntemp_rf.fit(X, y)\n\n# 获取最重要的10个特征的名称\nN_TOP_FEATURES = 10\nfeature_importances = pd.DataFrame({\n    'feature': X.columns,\n    'importance': temp_rf.feature_importances_\n}).sort_values('importance', ascending=False)\ntop_10_features = feature_importances['feature'].head(N_TOP_FEATURES).tolist()\n\n# 创建只包含这10个特征的新数据集\nX_top10 = X[top_10_features]\n\nprint(\"--- 最终优化流程 ---\")\nprint(f\"已成功筛选出最重要的 {N_TOP_FEATURES} 个特征用于最终调优:\")\nprint(top_10_features)\n\n\n# --- 2. 最终步骤二：在最佳特征集上进行贝叶斯优化 ---\n# The second step: run Bayesian Optimization on this optimal feature set.\n\nprint(\"\\n现在，我们只在这10个特征上进行贝叶斯优化 (Optuna)...\")\n\n# 定义目标函数，与之前类似，但这次使用的是筛选后的数据\ndef objective_top10(trial):\n    # 定义超参数搜索空间\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 200, 800),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n    }\n    \n    model = RandomForestClassifier(random_state=42, **params)\n    \n    # ！！！ 关键不同点 ！！！\n    # 注意：我们在这里使用 X_top10，而不是完整的 X\n    # CRITICAL DIFFERENCE: We are using X_top10 here, not the full X.\n    score = cross_val_score(model, X_top10, y, cv=5, scoring='roc_auc').mean()\n    \n    return score\n\n# 创建一个新的 \"研究\" (Study)\nstudy_top10 = optuna.create_study(direction='maximize', study_name='RF_Top10_Optimization')\n\n# 运行优化，n_trials 可以根据需要调整\nstudy_top10.optimize(objective_top10, n_trials=100)\n\n# --- 3. 打印最终优化结果 ---\n# Print the final optimization results\n\nbest_params_top10 = study_top10.best_params\nbest_score_top10 = study_top10.best_value\n\nprint(\"\\n在Top 10特征集上的贝叶斯优化完成!\")\nprint(f\"找到的最佳超参数组合 (Best Parameters): \\n{best_params_top10}\")\nprint(f\"\\n最终模型的最佳平均 AUC 分数是: {best_score_top10:.4f}\")\n\n# --- 4. 训练最终的、最优化的模型 ---\n# Train the final, most optimized model\n\nprint(\"\\n正在使用找到的最佳特征和最佳超参数来训练最终模型...\")\nfinal_model = RandomForestClassifier(random_state=42,  **best_params_top10)\nfinal_model.fit(X_top10, y)\n\nprint(\"最终模型训练完成！这个模型是我们整个分析过程的最终成果。\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n[I 2025-09-12 12:09:35,006] A new study created in memory with name: RF_Top10_Optimization\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n--- 最终优化流程 ---\n已成功筛选出最重要的 10 个特征用于最终调优:\n['SCL90R_OCS', 'CGI_Depressive', 'SCL90R_Somatization', 'PSP_Total_Score', 'CGI_Negative', 'Duration_of_Illness', 'CGI_Positive', 'CGI_Cognitive', 'Age', 'DIEPSS_Overall_Severity']\n\n现在，我们只在这10个特征上进行贝叶斯优化 (Optuna)...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n[I 2025-09-12 12:09:37,678] Trial 0 finished with value: 0.7974902882310043 and parameters: {'n_estimators': 709, 'max_depth': 7, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 0 with value: 0.7974902882310043.\n[I 2025-09-12 12:09:38,587] Trial 1 finished with value: 0.8014817339301008 and parameters: {'n_estimators': 271, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.8014817339301008.\n[I 2025-09-12 12:09:39,567] Trial 2 finished with value: 0.8006315628917478 and parameters: {'n_estimators': 293, 'max_depth': 4, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 1 with value: 0.8014817339301008.\n[I 2025-09-12 12:09:40,647] Trial 3 finished with value: 0.8014801450926372 and parameters: {'n_estimators': 322, 'max_depth': 4, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 1 with value: 0.8014817339301008.\n[I 2025-09-12 12:09:41,512] Trial 4 finished with value: 0.7991735820817267 and parameters: {'n_estimators': 222, 'max_depth': 9, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 1 with value: 0.8014817339301008.\n[I 2025-09-12 12:09:44,209] Trial 5 finished with value: 0.7990501135224367 and parameters: {'n_estimators': 748, 'max_depth': 7, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 1 with value: 0.8014817339301008.\n[I 2025-09-12 12:09:45,970] Trial 6 finished with value: 0.7985547616664362 and parameters: {'n_estimators': 502, 'max_depth': 5, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 1 with value: 0.8014817339301008.\n[I 2025-09-12 12:09:46,889] Trial 7 finished with value: 0.7994638309095616 and parameters: {'n_estimators': 258, 'max_depth': 5, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 1 with value: 0.8014817339301008.\n[I 2025-09-12 12:09:48,505] Trial 8 finished with value: 0.799350769235658 and parameters: {'n_estimators': 436, 'max_depth': 7, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.8014817339301008.\n[I 2025-09-12 12:09:51,306] Trial 9 finished with value: 0.7988488554809331 and parameters: {'n_estimators': 781, 'max_depth': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 1 with value: 0.8014817339301008.\n[I 2025-09-12 12:09:52,952] Trial 10 finished with value: 0.8026295736672434 and parameters: {'n_estimators': 545, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:09:54,757] Trial 11 finished with value: 0.8025121268019404 and parameters: {'n_estimators': 597, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:09:56,509] Trial 12 finished with value: 0.8025114277134563 and parameters: {'n_estimators': 583, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:09:58,411] Trial 13 finished with value: 0.8019518073820567 and parameters: {'n_estimators': 634, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:00,327] Trial 14 finished with value: 0.7980027677548615 and parameters: {'n_estimators': 496, 'max_depth': 10, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:02,236] Trial 15 finished with value: 0.802179201799835 and parameters: {'n_estimators': 613, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:03,651] Trial 16 finished with value: 0.7990156039727292 and parameters: {'n_estimators': 396, 'max_depth': 6, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:05,581] Trial 17 finished with value: 0.7979236913143022 and parameters: {'n_estimators': 545, 'max_depth': 8, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:07,601] Trial 18 finished with value: 0.8022298698265466 and parameters: {'n_estimators': 671, 'max_depth': 3, 'min_samples_leaf': 7, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:08,972] Trial 19 finished with value: 0.798337599361923 and parameters: {'n_estimators': 388, 'max_depth': 6, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:10,729] Trial 20 finished with value: 0.8015777156012718 and parameters: {'n_estimators': 536, 'max_depth': 4, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:12,487] Trial 21 finished with value: 0.8024547062160087 and parameters: {'n_estimators': 585, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:14,451] Trial 22 finished with value: 0.8019002496063656 and parameters: {'n_estimators': 654, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:16,000] Trial 23 finished with value: 0.8007322951869347 and parameters: {'n_estimators': 479, 'max_depth': 4, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:17,927] Trial 24 finished with value: 0.7993457802860224 and parameters: {'n_estimators': 562, 'max_depth': 5, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:20,111] Trial 25 finished with value: 0.8017842962482782 and parameters: {'n_estimators': 698, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:22,078] Trial 26 finished with value: 0.800559477336028 and parameters: {'n_estimators': 611, 'max_depth': 4, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8026295736672434.\n[I 2025-09-12 12:10:23,421] Trial 27 finished with value: 0.8026821959640351 and parameters: {'n_estimators': 443, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 27 with value: 0.8026821959640351.\n[I 2025-09-12 12:10:24,684] Trial 28 finished with value: 0.8006787831411634 and parameters: {'n_estimators': 367, 'max_depth': 5, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 27 with value: 0.8026821959640351.\n[I 2025-09-12 12:10:26,289] Trial 29 finished with value: 0.7973467049894263 and parameters: {'n_estimators': 444, 'max_depth': 6, 'min_samples_leaf': 6, 'max_features': 'sqrt'}. Best is trial 27 with value: 0.8026821959640351.\n[I 2025-09-12 12:10:27,720] Trial 30 finished with value: 0.8015735687354919 and parameters: {'n_estimators': 441, 'max_depth': 4, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 27 with value: 0.8026821959640351.\n[I 2025-09-12 12:10:29,317] Trial 31 finished with value: 0.8027416184851706 and parameters: {'n_estimators': 530, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 31 with value: 0.8027416184851706.\n[I 2025-09-12 12:10:30,882] Trial 32 finished with value: 0.8024012418353615 and parameters: {'n_estimators': 517, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'sqrt'}. Best is trial 31 with value: 0.8027416184851706.\n[I 2025-09-12 12:10:32,395] Trial 33 finished with value: 0.8017477370982424 and parameters: {'n_estimators': 465, 'max_depth': 4, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 31 with value: 0.8027416184851706.\n[I 2025-09-12 12:10:34,256] Trial 34 finished with value: 0.8003893605088093 and parameters: {'n_estimators': 573, 'max_depth': 4, 'min_samples_leaf': 10, 'max_features': 'sqrt'}. Best is trial 31 with value: 0.8027416184851706.\n[I 2025-09-12 12:10:35,841] Trial 35 finished with value: 0.8027395688848425 and parameters: {'n_estimators': 526, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 31 with value: 0.8027416184851706.\n[I 2025-09-12 12:10:36,969] Trial 36 finished with value: 0.8030340281319562 and parameters: {'n_estimators': 336, 'max_depth': 4, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 36 with value: 0.8030340281319562.\n[I 2025-09-12 12:10:38,038] Trial 37 finished with value: 0.8030425919658845 and parameters: {'n_estimators': 326, 'max_depth': 4, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 37 with value: 0.8030425919658845.\n[I 2025-09-12 12:10:39,100] Trial 38 finished with value: 0.80309461050444 and parameters: {'n_estimators': 324, 'max_depth': 4, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 38 with value: 0.80309461050444.\n[I 2025-09-12 12:10:40,241] Trial 39 finished with value: 0.799110981885664 and parameters: {'n_estimators': 325, 'max_depth': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 38 with value: 0.80309461050444.\n[I 2025-09-12 12:10:41,059] Trial 40 finished with value: 0.797393972903966 and parameters: {'n_estimators': 214, 'max_depth': 8, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 38 with value: 0.80309461050444.\n[I 2025-09-12 12:10:42,049] Trial 41 finished with value: 0.8029259395193131 and parameters: {'n_estimators': 296, 'max_depth': 4, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 38 with value: 0.80309461050444.\n[I 2025-09-12 12:10:42,992] Trial 42 finished with value: 0.8025842600227839 and parameters: {'n_estimators': 279, 'max_depth': 4, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 38 with value: 0.80309461050444.\n[I 2025-09-12 12:10:44,166] Trial 43 finished with value: 0.7979568503521658 and parameters: {'n_estimators': 314, 'max_depth': 5, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 38 with value: 0.80309461050444.\n[I 2025-09-12 12:10:44,999] Trial 44 finished with value: 0.8026418712692112 and parameters: {'n_estimators': 250, 'max_depth': 4, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 38 with value: 0.80309461050444.\n[I 2025-09-12 12:10:46,195] Trial 45 finished with value: 0.8011576110875435 and parameters: {'n_estimators': 342, 'max_depth': 5, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 38 with value: 0.80309461050444.\n[I 2025-09-12 12:10:47,015] Trial 46 finished with value: 0.8033251825971455 and parameters: {'n_estimators': 244, 'max_depth': 4, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:47,901] Trial 47 finished with value: 0.7975646934894207 and parameters: {'n_estimators': 240, 'max_depth': 5, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:49,013] Trial 48 finished with value: 0.7981127788608353 and parameters: {'n_estimators': 294, 'max_depth': 6, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:50,275] Trial 49 finished with value: 0.8000130761323249 and parameters: {'n_estimators': 358, 'max_depth': 5, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:51,239] Trial 50 finished with value: 0.8010956305380917 and parameters: {'n_estimators': 289, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:51,931] Trial 51 finished with value: 0.8011262315476388 and parameters: {'n_estimators': 206, 'max_depth': 4, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:53,237] Trial 52 finished with value: 0.8019321057975091 and parameters: {'n_estimators': 399, 'max_depth': 4, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:54,273] Trial 53 finished with value: 0.8028808165353493 and parameters: {'n_estimators': 313, 'max_depth': 4, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:55,207] Trial 54 finished with value: 0.79790157469681 and parameters: {'n_estimators': 264, 'max_depth': 5, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:56,410] Trial 55 finished with value: 0.7992486546518778 and parameters: {'n_estimators': 312, 'max_depth': 10, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:57,528] Trial 56 finished with value: 0.7999153785166933 and parameters: {'n_estimators': 336, 'max_depth': 4, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:58,413] Trial 57 finished with value: 0.7939838033908969 and parameters: {'n_estimators': 233, 'max_depth': 7, 'min_samples_leaf': 5, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:10:59,998] Trial 58 finished with value: 0.7960905065372718 and parameters: {'n_estimators': 415, 'max_depth': 6, 'min_samples_leaf': 3, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:11:00,978] Trial 59 finished with value: 0.8028706161988335 and parameters: {'n_estimators': 299, 'max_depth': 4, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:11:02,367] Trial 60 finished with value: 0.7986887959948585 and parameters: {'n_estimators': 369, 'max_depth': 8, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:11:03,353] Trial 61 finished with value: 0.8027565217805783 and parameters: {'n_estimators': 300, 'max_depth': 4, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:11:04,225] Trial 62 finished with value: 0.8017607337886942 and parameters: {'n_estimators': 265, 'max_depth': 4, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:11:05,399] Trial 63 finished with value: 0.8020949139723956 and parameters: {'n_estimators': 355, 'max_depth': 4, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:11:06,372] Trial 64 finished with value: 0.8008678071341981 and parameters: {'n_estimators': 278, 'max_depth': 5, 'min_samples_leaf': 6, 'max_features': 'log2'}. Best is trial 46 with value: 0.8033251825971455.\n[I 2025-09-12 12:11:07,358] Trial 65 finished with value: 0.8037308445783304 and parameters: {'n_estimators': 323, 'max_depth': 3, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:08,376] Trial 66 finished with value: 0.8028496117675659 and parameters: {'n_estimators': 334, 'max_depth': 3, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:09,515] Trial 67 finished with value: 0.8031817900160633 and parameters: {'n_estimators': 377, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:10,677] Trial 68 finished with value: 0.8031838396163911 and parameters: {'n_estimators': 383, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:11,888] Trial 69 finished with value: 0.8032432621375266 and parameters: {'n_estimators': 401, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:13,046] Trial 70 finished with value: 0.8030710957099799 and parameters: {'n_estimators': 382, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:14,199] Trial 71 finished with value: 0.802961704250617 and parameters: {'n_estimators': 381, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:15,451] Trial 72 finished with value: 0.8027349135910746 and parameters: {'n_estimators': 413, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:16,687] Trial 73 finished with value: 0.8027929379352422 and parameters: {'n_estimators': 409, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:17,943] Trial 74 finished with value: 0.8034633955680966 and parameters: {'n_estimators': 386, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:19,269] Trial 75 finished with value: 0.8027976885592581 and parameters: {'n_estimators': 425, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:20,425] Trial 76 finished with value: 0.802961704250617 and parameters: {'n_estimators': 381, 'max_depth': 3, 'min_samples_leaf': 8, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:21,493] Trial 77 finished with value: 0.8033472038843898 and parameters: {'n_estimators': 353, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:22,903] Trial 78 finished with value: 0.802961005162133 and parameters: {'n_estimators': 466, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:24,004] Trial 79 finished with value: 0.8034598524605532 and parameters: {'n_estimators': 364, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:25,125] Trial 80 finished with value: 0.8035132691760765 and parameters: {'n_estimators': 370, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:26,199] Trial 81 finished with value: 0.8033472038843898 and parameters: {'n_estimators': 353, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:27,270] Trial 82 finished with value: 0.8035166692882484 and parameters: {'n_estimators': 352, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:28,339] Trial 83 finished with value: 0.8035166692882484 and parameters: {'n_estimators': 352, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:29,415] Trial 84 finished with value: 0.8031216684064437 and parameters: {'n_estimators': 354, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:30,469] Trial 85 finished with value: 0.8032945339224742 and parameters: {'n_estimators': 347, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 65 with value: 0.8037308445783304.\n[I 2025-09-12 12:11:31,578] Trial 86 finished with value: 0.8037974804215503 and parameters: {'n_estimators': 363, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 86 with value: 0.8037974804215503.\n[I 2025-09-12 12:11:32,688] Trial 87 finished with value: 0.8037394560773828 and parameters: {'n_estimators': 367, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 86 with value: 0.8037974804215503.\n[I 2025-09-12 12:11:33,800] Trial 88 finished with value: 0.8032297093539629 and parameters: {'n_estimators': 367, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 86 with value: 0.8037974804215503.\n[I 2025-09-12 12:11:35,173] Trial 89 finished with value: 0.8025173381888207 and parameters: {'n_estimators': 455, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 86 with value: 0.8037974804215503.\n[I 2025-09-12 12:11:36,467] Trial 90 finished with value: 0.8026294306718716 and parameters: {'n_estimators': 428, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 86 with value: 0.8037974804215503.\n[I 2025-09-12 12:11:37,563] Trial 91 finished with value: 0.8035200217352966 and parameters: {'n_estimators': 362, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 86 with value: 0.8037974804215503.\n[I 2025-09-12 12:11:38,750] Trial 92 finished with value: 0.8026253791363398 and parameters: {'n_estimators': 393, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 86 with value: 0.8037974804215503.\n[I 2025-09-12 12:11:39,863] Trial 93 finished with value: 0.8032297093539629 and parameters: {'n_estimators': 367, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 86 with value: 0.8037974804215503.\n[I 2025-09-12 12:11:40,899] Trial 94 finished with value: 0.803572739362336 and parameters: {'n_estimators': 341, 'max_depth': 3, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 86 with value: 0.8037974804215503.\n[I 2025-09-12 12:11:41,860] Trial 95 finished with value: 0.8043518893660696 and parameters: {'n_estimators': 316, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 95 with value: 0.8043518893660696.\n[I 2025-09-12 12:11:42,822] Trial 96 finished with value: 0.8036294131946595 and parameters: {'n_estimators': 314, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 95 with value: 0.8043518893660696.\n[I 2025-09-12 12:11:43,779] Trial 97 finished with value: 0.8041311045121395 and parameters: {'n_estimators': 315, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 95 with value: 0.8043518893660696.\n[I 2025-09-12 12:11:44,765] Trial 98 finished with value: 0.8039075709693974 and parameters: {'n_estimators': 325, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 95 with value: 0.8043518893660696.\n[I 2025-09-12 12:11:45,707] Trial 99 finished with value: 0.8032945339224742 and parameters: {'n_estimators': 310, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 95 with value: 0.8043518893660696.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n在Top 10特征集上的贝叶斯优化完成!\n找到的最佳超参数组合 (Best Parameters): \n{'n_estimators': 316, 'max_depth': 3, 'min_samples_leaf': 10, 'max_features': 'log2'}\n\n最终模型的最佳平均 AUC 分数是: 0.8044\n\n正在使用找到的最佳特征和最佳超参数来训练最终模型...\n最终模型训练完成！这个模型是我们整个分析过程的最终成果。\n```\n:::\n:::\n\n\n同样运行了好几次，得到了较好的数据，***是最终模型的最佳平均 AUC 分数是: 0.8044***\n\n***AUC 接近 0.805 通常意味着模型已经捕捉到了数据中大部分的可预测结构。这就是我们在机器学习中常说的“性能瓶颈”：再怎么调参、选特征，提升空间已经非常有限，这是我们随机森林的极限了，除非换一个更加适配这个数据的机器学习模型。***\n\n***模型的性能上限，往往不是由算法决定的，而是由数据本身的可预测性决定的。 如果数据中没有更多“可解释的信号”，再复杂的模型也无法凭空创造预测能力。***\n\n#### 1. 概念引入：性能天花板与不可约减误差 (Irreducible Error)🧉\n\n性能天花板（Performance Ceiling）。”\n\n**核心知识点：**\\\n在机器学习中，一个模型的总误差可以被分解为两部分：\n\n**总误差 = 可约减误差 (Reducible Error) + 不可约减误差 (Irreducible Error)**\n\n-   **可约减误差：** 这是我们可以通过选择更好的模型、做特征工程、调优超参数来减少的部分。我们之前做的所有工作，都是在努力**将这部分误差降到最低**。\n\n-   **不可约减误差：** 这是数据本身固有的、无法被任何模型消除的“噪声”或不确定性。**这个误差决定了我们性能天花板的高度。**\n\n-   **打个比方 (Analogy):**\\\n    这就像用麦克风录音。我们可以买更好的麦克风、调整录音棚的环境来减少背景噪音（降低**可约减误差**）。但是，声音本身的一些微小颤动、空气中的随机分子运动是无法被消除的（**不可约减误差**）。我们能做的，就是尽可能清晰地捕捉到“信号”，但我们无法消除所有的“噪声”。\n\n#### 教学总结：当我们触及天花板时，该做什么🤖？\n\n这是最关键的教学部分。当模型调优不再有效时，真正的数据科学家会从以下几个方面思考，这也是未来的工作方向：\n\n**路径 A：获取更好的数据 (数据为中心 - The Data-Centric Approach)**\\\n这是**最重要、最有效**的路径。\n\n-   **行动：** 与领域专家（精神科医生、心理学家）合作，讨论是否可以收集新的、可能与主观幸福感相关的数据。比如，在下一次研究中加入“社会支持量表”、“生活事件记录”等。\n\n-   **结论：** **提升性能天花板的最好方法，是为模型提供更高信噪比、更丰富信息的数据。**\n\n**路径 B：尝试完全不同的模型 (模型为中心 - The Model-Centric Approach)**\n\n-   **行动：** 我们已经证明了随机森林（一种基于决策树的集成模型）的极限。我们可以尝试结构完全不同的模型，比如：\n\n    -   **深度学习/神经网络 (Deep Learning / Neural Networks):** 如果有***足够多的数据***，神经网络能够捕捉到比树模型更高阶、更复杂的非线性关系。\n\n    -   **梯度提升机 (Gradient Boosting Machines):** 我们可以对之前测试过的 XGBoost 或 LightGBM 模型进行同样深入的特征选择和贝叶斯优化，有时它们在特定数据集上的表现会略胜于随机森林。\n\n-   **结论：** 换一个更强大的模型**有可能**会略微提升性能，但这通常是在数据质量已经很好的前提下。如果数据本身信息量有限，换模型也只是“锦上添花”，而不会有质的飞跃。\n\n------------------------------------------------------------------------\n\n## 论文总结🤗\n\nthis is what it is 没什么总结的，**我们的机器学习得到了在我们测量的所有事物中，哪些与患者的主观幸福感最密切相关？即 OCS、躯体化、认知和抑郁是最重要的预测因素。我们确定了主要特征**\n\n***`这也是差不多你们要写的论文的机器学习的大概流程`***\n\n-   但是如果看我们的复现论文，他们接下来的步骤是\n\n### Step 1: Network Analysis (网络分析)🫥\n\n**“为什么”（目的）：**\\\n机器学习告诉我们，强迫症症状、躯体化症状、抑郁症和认知是最重要的预测因素。但它并没有告诉我们它们是如何相互关联的。强迫症症状和躯体化症状是否高度相关？抑郁症是否是连接一切的中心枢纽？\n\n网络分析帮助我们将症状的“生态系统”可视化。它将每个变量（症状、人口统计因素）视为一个“节点”（城市），并将它们之间的相关性视为一条“边”（高速公路）。相关性越强，高速公路就越粗。\n\n论文中的图3）：\n\n-   **社群（群落）：** 他们寻找的是节点之间的连接比与网络其他部分的连接更紧密的集群。在论文中，他们发现了一个关键模式：\n\n    -   **社区 1：** 主观幸福感（SWB）、强迫症\n\n    -   **社区 2：** 精神病症状（阳性、阴性）、认知缺陷和社会心理功能 (PSP) 形成了另一个不同的集群。\n\n    -   幸福感的内在体验与强迫症和躯体系统更密切相关\n\n-   **中心性：** 该指标用于识别网络中最具“影响力”的节点。论文指出， **抑郁症**的“中介中心性”最高。\n\n    -   **这是什么意思？** 这意味着抑郁症是连接两个群体的关键**桥梁** 。要从“精神病症状”群体走向“主观幸福感”群体，通常需要经过“抑郁症”这一环节。这表明抑郁症起着关键的中介作用。\n\n这是一种可视化和探索性的工具。它从我们的机器学习模型中获取重要特征列表，并将其转化为地图。这张地图向我们展示了复杂症状网络的结构和关键因素，这让我们假设抑郁症是一个关键的中介因素。\n\n### Step 2: Structural Equation Modeling (SEM) (结构方程模型)🍔\n\n方向或因果关系。它显示了两座城市之间的高速公路，但没有显示交通走向\n\n看看论文中的图 4。这是他们的 SEM 分析结果。它就像一个流程图，展示了影响力的流动。\n\n-   **直接效应（直接效应）：** 从一个变量到另一个变量的直线箭头。\n\n    -   论文发现， **强迫性行为对主观幸福感（SWB）的直接负面影响最为强烈** （见粗红色箭头，系数为-0.59）。这是论文最重要的发现：高强迫性行为会直接摧毁患者的幸福感。\n\n-   **间接效应：** 至少经过一个其他变量的路径。\n\n    -   论文证实了抑郁症的“桥梁”作用。例如，阴性症状有一个箭头指向抑郁症，抑郁症也有一个箭头指向主观幸福感。这是一条**间接路径** 。这意味着阴性症状部分地通过增加抑郁来恶化主观幸福感 。\n\n    -   OCS 还通过抑郁症对 SWB 产生间接影响 。\n\nSEM 是一种验证性工具，是分析的最终步骤，也是最有力的一步。它超越了“什么是重要的”（ML）和“它们如何关联”（网络分析）的范畴，从而检验了关于“什么导致什么”的特定理论。SEM 结果为本文的主要结论提供了最有力的证据：管理 OCS 对于改善精神分裂症患者的主观幸福感至关重要，这既是因为它强大的直接影响，也是因为它通过抑郁症产生的间接影响。\n\n# 我滴任务完成了🛫\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}